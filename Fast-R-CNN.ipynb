{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WVa8hALF831",
        "outputId": "e57f8f12-5fea-4784-cdf3-85cd8c2176f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Label data for 76 (9).jpg:\n",
            "1 0.661385 0.353873 0.127934 0.181925\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define path to your dataset (images and labels are directly in the 'augmented' folder)\n",
        "dataset_path = '../brain_tumor/data/augmented/'\n",
        "\n",
        "# List all image files (assuming .jpg images)\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Check first image and its label\n",
        "image_file = image_files[0]\n",
        "image_path = os.path.join(dataset_path, image_file)\n",
        "label_path = os.path.join(dataset_path, image_file.replace('.jpg', '.txt'))\n",
        "\n",
        "# Open the image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Read the label\n",
        "with open(label_path, 'r') as f:\n",
        "    label_data = f.readlines()\n",
        "\n",
        "# Show image and print label data\n",
        "image.show()\n",
        "print(f\"Label data for {image_file}:\")\n",
        "for line in label_data:\n",
        "    print(line.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "thH63fpN22it"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = '../brain_tumor/data/augmented/'\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for filename in os.listdir(dataset_path):\n",
        "    # Check if the file is an image\n",
        "    if filename.endswith('.jpg'):\n",
        "        # Corresponding label file for the image\n",
        "        label_file = filename.replace('.jpg', '.txt')\n",
        "        label_path = os.path.join(dataset_path, label_file)\n",
        "\n",
        "        # If no corresponding label file exists, delete the image\n",
        "        if not os.path.exists(label_path):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            os.remove(image_path)\n",
        "            print(f\"Deleted image: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2D9IbvEpOQew"
      },
      "outputs": [],
      "source": [
        "# Convert YOLO format to Faster R-CNN format\n",
        "def convert_yolo_to_fasterrcnn(label_data, image_width, image_height):\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for line in label_data:\n",
        "        parts = list(map(float, line.strip().split()))\n",
        "        label = int(parts[0])\n",
        "        if label == 0:\n",
        "            continue  # Skip no-tumor images\n",
        "\n",
        "        # Extract YOLO format: x_center, y_center, width, height\n",
        "        x_center, y_center, width, height = parts[1:]\n",
        "\n",
        "        # Convert YOLO to pixel coordinates\n",
        "        xmin = (x_center - width / 2) * image_width\n",
        "        xmax = (x_center + width / 2) * image_width\n",
        "        ymin = (y_center - height / 2) * image_height\n",
        "        ymax = (y_center + height / 2) * image_height\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label)  # 1 for tumor\n",
        "\n",
        "    return boxes, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRGiO_EsPOZh",
        "outputId": "4cd79655-224b-481c-ecc8-d1e032234ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 878\n",
            "Example target (bounding boxes and labels) for the first image: {'boxes': tensor([[122.5917, 125.5962, 216.9390, 207.9247]]), 'labels': tensor([1])}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Function to load all images and labels, and convert YOLO to Faster R-CNN format\n",
        "def load_and_convert(dataset_path):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    # List all image files in the directory\n",
        "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Get the corresponding label file\n",
        "        label_file = image_file.replace('.jpg', '.txt')\n",
        "\n",
        "        image_path = os.path.join(dataset_path, image_file)\n",
        "        label_path = os.path.join(dataset_path, label_file)\n",
        "\n",
        "        # Open image\n",
        "        image = Image.open(image_path)\n",
        "        image_width, image_height = image.size\n",
        "\n",
        "        # Read label data\n",
        "        with open(label_path, 'r') as f:\n",
        "            label_data = f.readlines()\n",
        "\n",
        "        # Convert YOLO to Faster R-CNN format\n",
        "        boxes, labels = convert_yolo_to_fasterrcnn(label_data, image_width, image_height)\n",
        "\n",
        "        # If no boxes were returned, add a \"dummy\" empty box and label (no tumor)\n",
        "        if len(boxes) == 0:\n",
        "            # Add an empty box and label\n",
        "            boxes = [[0.0, 0.0, 0.0, 0.0]]  # This is an \"empty\" box\n",
        "            labels = [0]  # Label 0 means no tumor\n",
        "\n",
        "        # Store the image and its corresponding target (bounding boxes and labels)\n",
        "        images.append(image)\n",
        "        targets.append({'boxes': torch.tensor(boxes, dtype=torch.float32), 'labels': torch.tensor(labels, dtype=torch.int64)})\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "# Load and convert all images and labels\n",
        "dataset_path = '../brain_tumor/data/augmented/'\n",
        "images, targets = load_and_convert(dataset_path)\n",
        "\n",
        "# Example: Print the number of images and check a target\n",
        "print(f\"Number of images: {len(images)}\")\n",
        "print(f\"Example target (bounding boxes and labels) for the first image: {targets[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiQqLYRERj0t",
        "outputId": "a1f68334-722a-4c81-8e7a-87497ed52615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 878\n",
            "Example target (bounding boxes and labels) for the first image: {'boxes': tensor([[0., 0., 0., 0.]]), 'labels': tensor([0])}\n"
          ]
        }
      ],
      "source": [
        "# Example: Print the number of images and check a target\n",
        "print(f\"Number of images: {len(images)}\")\n",
        "print(f\"Example target (bounding boxes and labels) for the first image: {targets[9]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSrl6sK1hRax",
        "outputId": "f2349ff6-741d-44fb-d11f-f30d18f74efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 702\n",
            "Testing dataset size: 176\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define the split ratio\n",
        "train_size = int(0.8 * len(images))\n",
        "test_size = len(images) - train_size\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_images, test_images = random_split(images, [train_size, test_size])\n",
        "train_targets, test_targets = random_split(targets, [train_size, test_size])\n",
        "\n",
        "# You can optionally check the sizes of the datasets\n",
        "print(f\"Training dataset size: {len(train_images)}\")\n",
        "print(f\"Testing dataset size: {len(test_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hK44uU4whgpZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, images, targets, transform=None):\n",
        "        self.images = images\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image and its corresponding target (boxes and labels)\n",
        "        image = self.images[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        # Ensure target is a dictionary with 'boxes' and 'labels'\n",
        "        target = {\n",
        "            'boxes': target['boxes'].float(),\n",
        "    'labels': target['labels'].long()\n",
        "        }\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zinLFAF0hn_m"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define the transformations for grayscale images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((800, 800)),  # Resize images to a fixed size (optional)\n",
        "    transforms.Grayscale(num_output_channels=1),  # Ensure the image is grayscale (single channel)\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize with grayscale values\n",
        "])\n",
        "\n",
        "# Apply transformations to the images\n",
        "train_dataset = BrainTumorDataset(images=train_images, targets=train_targets, transform=transform)\n",
        "test_dataset = BrainTumorDataset(images=test_images, targets=test_targets, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eW3aTfFRDAPH"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]  # keep as list of dicts\n",
        "    return torch.stack(images, dim=0), targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-7-V9wwJhq7q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0V_zjQFdkfzD"
      },
      "outputs": [],
      "source": [
        "for images_batch, targets_batch in train_loader:\n",
        "    boxes_first_img = targets_batch[0]['boxes']  # Tensor [N, 4]\n",
        "    labels_first_img = targets_batch[0]['labels']  # Tensor [N]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PBXl_t9iL-W",
        "outputId": "0c1d3522-af9b-40b6-b6c2-b7f118d724bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.9835421442985535\n",
            "Epoch 1, Loss: 0.9242234826087952\n",
            "Epoch 1, Loss: 0.985794186592102\n",
            "Epoch 1, Loss: 0.8307316899299622\n",
            "Epoch 1, Loss: 0.9542766213417053\n",
            "Epoch 1, Loss: 0.9417815208435059\n",
            "Epoch 1, Loss: 0.8876802325248718\n",
            "Epoch 1, Loss: 0.8934490084648132\n",
            "Epoch 1, Loss: 0.923782229423523\n",
            "Epoch 1, Loss: 0.8899625539779663\n",
            "Epoch 1, Loss: 1.1418803930282593\n",
            "Epoch 1, Loss: 0.9858635663986206\n",
            "Epoch 1, Loss: 0.9130094647407532\n",
            "Epoch 1, Loss: 0.8722971081733704\n",
            "Epoch 1, Loss: 0.9042437076568604\n",
            "Epoch 1, Loss: 0.892036497592926\n",
            "Epoch 1, Loss: 0.8992630839347839\n",
            "Epoch 1, Loss: 0.9612622261047363\n",
            "Epoch 1, Loss: 0.865350067615509\n",
            "Epoch 1, Loss: 1.0098448991775513\n",
            "Epoch 1, Loss: 0.9217138290405273\n",
            "Epoch 1, Loss: 0.8912782073020935\n",
            "Epoch 1, Loss: 1.0129318237304688\n",
            "Epoch 1, Loss: 0.8902310729026794\n",
            "Epoch 1, Loss: 0.8787598013877869\n",
            "Epoch 1, Loss: 0.8688774108886719\n",
            "Epoch 1, Loss: 0.9241093993186951\n",
            "Epoch 1, Loss: 0.8839316964149475\n",
            "Epoch 1, Loss: 0.9152483344078064\n",
            "Epoch 1, Loss: 0.9019166231155396\n",
            "Epoch 1, Loss: 0.8640068173408508\n",
            "Epoch 1, Loss: 0.9324870109558105\n",
            "Epoch 1, Loss: 0.9102707505226135\n",
            "Epoch 1, Loss: 0.8471910953521729\n",
            "Epoch 1, Loss: 0.9254953265190125\n",
            "Epoch 1, Loss: 1.0441128015518188\n",
            "Epoch 1, Loss: 1.0707460641860962\n",
            "Epoch 1, Loss: 0.8917248845100403\n",
            "Epoch 1, Loss: 0.9062881469726562\n",
            "Epoch 1, Loss: 0.8810921311378479\n",
            "Epoch 1, Loss: 0.9202988743782043\n",
            "Epoch 1, Loss: 0.899333655834198\n",
            "Epoch 1, Loss: 1.0040990114212036\n",
            "Epoch 1, Loss: 0.9161660671234131\n",
            "Epoch 1, Loss: 0.8909147381782532\n",
            "Epoch 1, Loss: 0.8977131247520447\n",
            "Epoch 1, Loss: 0.8820016384124756\n",
            "Epoch 1, Loss: 0.9017696380615234\n",
            "Epoch 1, Loss: 0.8924925923347473\n",
            "Epoch 1, Loss: 0.8947693109512329\n",
            "Epoch 1, Loss: 0.9254158139228821\n",
            "Epoch 1, Loss: 0.8784365057945251\n",
            "Epoch 1, Loss: 0.8592660427093506\n",
            "Epoch 1, Loss: 0.8685771822929382\n",
            "Epoch 1, Loss: 0.8915634751319885\n",
            "Epoch 1, Loss: 0.8914934992790222\n",
            "Epoch 1, Loss: 0.8638612031936646\n",
            "Epoch 1, Loss: 0.8942046165466309\n",
            "Epoch 1, Loss: 0.9295443892478943\n",
            "Epoch 1, Loss: 0.8963398337364197\n",
            "Epoch 1, Loss: 0.9176263809204102\n",
            "Epoch 1, Loss: 0.8424478769302368\n",
            "Epoch 1, Loss: 0.8357892036437988\n",
            "Epoch 1, Loss: 0.9091858267784119\n",
            "Epoch 1, Loss: 0.9319076538085938\n",
            "Epoch 1, Loss: 0.9400706887245178\n",
            "Epoch 1, Loss: 0.9957651495933533\n",
            "Epoch 1, Loss: 0.9475980997085571\n",
            "Epoch 1, Loss: 0.9268659353256226\n",
            "Epoch 1, Loss: 0.9082463979721069\n",
            "Epoch 1, Loss: 0.864028811454773\n",
            "Epoch 1, Loss: 0.9246311187744141\n",
            "Epoch 1, Loss: 0.9406011700630188\n",
            "Epoch 1, Loss: 0.8814513087272644\n",
            "Epoch 1, Loss: 0.8729591369628906\n",
            "Epoch 1, Loss: 0.9241880178451538\n",
            "Epoch 1, Loss: 0.9008916020393372\n",
            "Epoch 1, Loss: 1.0220927000045776\n",
            "Epoch 1, Loss: 0.8931001424789429\n",
            "Epoch 1, Loss: 0.9229544401168823\n",
            "Epoch 1, Loss: 0.9237501621246338\n",
            "Epoch 1, Loss: 0.98581463098526\n",
            "Epoch 1, Loss: 1.0067895650863647\n",
            "Epoch 1, Loss: 0.8491612672805786\n",
            "Epoch 1, Loss: 1.081756830215454\n",
            "Epoch 1, Loss: 0.9722092151641846\n",
            "Epoch 1, Loss: 0.9555447697639465\n",
            "Epoch 1, Loss: 0.8911319971084595\n",
            "Epoch 1, Loss: 0.8466220498085022\n",
            "Epoch 1, Loss: 0.9923896789550781\n",
            "Epoch 1, Loss: 0.8792285323143005\n",
            "Epoch 1, Loss: 0.9851363897323608\n",
            "Epoch 1, Loss: 0.8871288299560547\n",
            "Epoch 1, Loss: 0.9166443347930908\n",
            "Epoch 1, Loss: 0.9170157313346863\n",
            "Epoch 1, Loss: 0.9489265084266663\n",
            "Epoch 1, Loss: 0.8692851662635803\n",
            "Epoch 1, Loss: 1.2223135232925415\n",
            "Epoch 1, Loss: 0.9305704236030579\n",
            "Epoch 1, Loss: 0.8504453897476196\n",
            "Epoch 1, Loss: 0.948762059211731\n",
            "Epoch 1, Loss: 0.9165202379226685\n",
            "Epoch 1, Loss: 0.9203547835350037\n",
            "Epoch 1, Loss: 0.9102023243904114\n",
            "Epoch 1, Loss: 0.9309231638908386\n",
            "Epoch 1, Loss: 0.9114533066749573\n",
            "Epoch 1, Loss: 0.9227616786956787\n",
            "Epoch 1, Loss: 1.0164605379104614\n",
            "Epoch 1, Loss: 0.8823056221008301\n",
            "Epoch 1, Loss: 0.8910817503929138\n",
            "Epoch 1, Loss: 0.942661464214325\n",
            "Epoch 1, Loss: 0.9024132490158081\n",
            "Epoch 1, Loss: 0.9432089924812317\n",
            "Epoch 1, Loss: 0.9170201420783997\n",
            "Epoch 1, Loss: 0.922312319278717\n",
            "Epoch 1, Loss: 0.876011312007904\n",
            "Epoch 1, Loss: 0.9453791975975037\n",
            "Epoch 1, Loss: 0.9526651501655579\n",
            "Epoch 1, Loss: 1.122002124786377\n",
            "Epoch 1, Loss: 0.9018927216529846\n",
            "Epoch 1, Loss: 0.8861780762672424\n",
            "Epoch 1, Loss: 0.9618245363235474\n",
            "Epoch 1, Loss: 0.8957896828651428\n",
            "Epoch 1, Loss: 0.8592974543571472\n",
            "Epoch 1, Loss: 0.8983465433120728\n",
            "Epoch 1, Loss: 0.8760064244270325\n",
            "Epoch 1, Loss: 0.9266040325164795\n",
            "Epoch 1, Loss: 0.9109610319137573\n",
            "Epoch 1, Loss: 0.9653055667877197\n",
            "Epoch 1, Loss: 0.9920694828033447\n",
            "Epoch 1, Loss: 0.8705257177352905\n",
            "Epoch 1, Loss: 0.8588431477546692\n",
            "Epoch 1, Loss: 1.0407257080078125\n",
            "Epoch 1, Loss: 0.8875666260719299\n",
            "Epoch 1, Loss: 0.8752537369728088\n",
            "Epoch 1, Loss: 0.8960320353507996\n",
            "Epoch 1, Loss: 0.8751791715621948\n",
            "Epoch 1, Loss: 0.9629327058792114\n",
            "Epoch 1, Loss: 0.9383366107940674\n",
            "Epoch 1, Loss: 0.8923901915550232\n",
            "Epoch 1, Loss: 0.8703136444091797\n",
            "Epoch 1, Loss: 1.055264949798584\n",
            "Epoch 1, Loss: 1.1269621849060059\n",
            "Epoch 1, Loss: 0.8602091670036316\n",
            "Epoch 1, Loss: 0.8396160006523132\n",
            "Epoch 1, Loss: 0.9052733182907104\n",
            "Epoch 1, Loss: 0.8963305354118347\n",
            "Epoch 1, Loss: 0.9254217743873596\n",
            "Epoch 1, Loss: 0.9292806386947632\n",
            "Epoch 1, Loss: 0.9144656658172607\n",
            "Epoch 1, Loss: 0.8469319343566895\n",
            "Epoch 1, Loss: 0.9776195883750916\n",
            "Epoch 1, Loss: 0.9083209037780762\n",
            "Epoch 1, Loss: 0.947502076625824\n",
            "Epoch 1, Loss: 0.8917434215545654\n",
            "Epoch 1, Loss: 0.8668145537376404\n",
            "Epoch 1, Loss: 1.0339231491088867\n",
            "Epoch 1, Loss: 0.8750483393669128\n",
            "Epoch 1, Loss: 0.8947218656539917\n",
            "Epoch 1, Loss: 0.908963143825531\n",
            "Epoch 1, Loss: 0.9246239066123962\n",
            "Epoch 1, Loss: 0.8895711302757263\n",
            "Epoch 1, Loss: 0.8842476010322571\n",
            "Epoch 1, Loss: 0.9053208827972412\n",
            "Epoch 1, Loss: 0.8822340965270996\n",
            "Epoch 1, Loss: 0.9325960278511047\n",
            "Epoch 1, Loss: 1.0136445760726929\n",
            "Epoch 1, Loss: 1.015571117401123\n",
            "Epoch 1, Loss: 0.9353191256523132\n",
            "Epoch 1, Loss: 0.9003618955612183\n",
            "Epoch 1, Loss: 0.9716920256614685\n",
            "Epoch 1, Loss: 0.94465571641922\n",
            "Epoch 1, Loss: 1.0121257305145264\n",
            "Epoch 1, Loss: 1.032152533531189\n",
            "Epoch 1, Loss: 0.8494154214859009\n",
            "Epoch 1, Loss: 0.9429028034210205\n",
            "Epoch 2, Loss: 1.1220207214355469\n",
            "Epoch 2, Loss: 0.9041833281517029\n",
            "Epoch 2, Loss: 0.898545503616333\n",
            "Epoch 2, Loss: 0.8724231123924255\n",
            "Epoch 2, Loss: 0.948492169380188\n",
            "Epoch 2, Loss: 1.1336058378219604\n",
            "Epoch 2, Loss: 0.8815045356750488\n",
            "Epoch 2, Loss: 0.9575900435447693\n",
            "Epoch 2, Loss: 0.9039267301559448\n",
            "Epoch 2, Loss: 0.8907285332679749\n",
            "Epoch 2, Loss: 0.9024427533149719\n",
            "Epoch 2, Loss: 0.8746594190597534\n",
            "Epoch 2, Loss: 0.9255998730659485\n",
            "Epoch 2, Loss: 0.9782850742340088\n",
            "Epoch 2, Loss: 0.9241055250167847\n",
            "Epoch 2, Loss: 0.8865599036216736\n",
            "Epoch 2, Loss: 0.9141266345977783\n",
            "Epoch 2, Loss: 0.9103027582168579\n",
            "Epoch 2, Loss: 1.1136541366577148\n",
            "Epoch 2, Loss: 1.1681522130966187\n",
            "Epoch 2, Loss: 0.9685437083244324\n",
            "Epoch 2, Loss: 0.9187445044517517\n",
            "Epoch 2, Loss: 0.9805922508239746\n",
            "Epoch 2, Loss: 0.916965126991272\n",
            "Epoch 2, Loss: 0.9550744295120239\n",
            "Epoch 2, Loss: 1.064124584197998\n",
            "Epoch 2, Loss: 0.9146972298622131\n",
            "Epoch 2, Loss: 0.9470224976539612\n",
            "Epoch 2, Loss: 0.8572329878807068\n",
            "Epoch 2, Loss: 0.9035598635673523\n",
            "Epoch 2, Loss: 0.8941718339920044\n",
            "Epoch 2, Loss: 0.9341497421264648\n",
            "Epoch 2, Loss: 0.9605088233947754\n",
            "Epoch 2, Loss: 0.9183963537216187\n",
            "Epoch 2, Loss: 0.9259536266326904\n",
            "Epoch 2, Loss: 0.8646358251571655\n",
            "Epoch 2, Loss: 0.8645250201225281\n",
            "Epoch 2, Loss: 0.8903857469558716\n",
            "Epoch 2, Loss: 0.9491041302680969\n",
            "Epoch 2, Loss: 0.8798840641975403\n",
            "Epoch 2, Loss: 0.9386518597602844\n",
            "Epoch 2, Loss: 0.9541273713111877\n",
            "Epoch 2, Loss: 1.0383038520812988\n",
            "Epoch 2, Loss: 0.9286456108093262\n",
            "Epoch 2, Loss: 0.9242726564407349\n",
            "Epoch 2, Loss: 0.9082918167114258\n",
            "Epoch 2, Loss: 0.926628053188324\n",
            "Epoch 2, Loss: 0.9063994288444519\n",
            "Epoch 2, Loss: 0.8932561278343201\n",
            "Epoch 2, Loss: 0.8804217576980591\n",
            "Epoch 2, Loss: 1.0229367017745972\n",
            "Epoch 2, Loss: 0.8788759112358093\n",
            "Epoch 2, Loss: 0.8394602537155151\n",
            "Epoch 2, Loss: 0.912566065788269\n",
            "Epoch 2, Loss: 0.9017148017883301\n",
            "Epoch 2, Loss: 0.8980393409729004\n",
            "Epoch 2, Loss: 0.905701220035553\n",
            "Epoch 2, Loss: 0.8999239206314087\n",
            "Epoch 2, Loss: 0.9163216352462769\n",
            "Epoch 2, Loss: 0.9340224266052246\n",
            "Epoch 2, Loss: 0.960307776927948\n",
            "Epoch 2, Loss: 0.9024017453193665\n",
            "Epoch 2, Loss: 0.9467101693153381\n",
            "Epoch 2, Loss: 0.9207959175109863\n",
            "Epoch 2, Loss: 0.9374248385429382\n",
            "Epoch 2, Loss: 0.8815850019454956\n",
            "Epoch 2, Loss: 0.9287950992584229\n",
            "Epoch 2, Loss: 0.9551299810409546\n",
            "Epoch 2, Loss: 0.898190975189209\n",
            "Epoch 2, Loss: 0.8746376037597656\n",
            "Epoch 2, Loss: 0.8971659541130066\n",
            "Epoch 2, Loss: 1.01163649559021\n",
            "Epoch 2, Loss: 0.8611109852790833\n",
            "Epoch 2, Loss: 0.8935591578483582\n",
            "Epoch 2, Loss: 0.9065515995025635\n",
            "Epoch 2, Loss: 1.1058781147003174\n",
            "Epoch 2, Loss: 0.905957818031311\n",
            "Epoch 2, Loss: 0.9165067076683044\n",
            "Epoch 2, Loss: 0.9385257363319397\n",
            "Epoch 2, Loss: 1.1011098623275757\n",
            "Epoch 2, Loss: 0.8959333896636963\n",
            "Epoch 2, Loss: 1.0518202781677246\n",
            "Epoch 2, Loss: 1.032904028892517\n",
            "Epoch 2, Loss: 0.8833416700363159\n",
            "Epoch 2, Loss: 0.8752273917198181\n",
            "Epoch 2, Loss: 0.8640240430831909\n",
            "Epoch 2, Loss: 0.9192742109298706\n",
            "Epoch 2, Loss: 0.9530050158500671\n",
            "Epoch 2, Loss: 0.8656567931175232\n",
            "Epoch 2, Loss: 0.9488218426704407\n",
            "Epoch 2, Loss: 0.9030937552452087\n",
            "Epoch 2, Loss: 0.9739463925361633\n",
            "Epoch 2, Loss: 0.9534770250320435\n",
            "Epoch 2, Loss: 0.8876196146011353\n",
            "Epoch 2, Loss: 0.9434930086135864\n",
            "Epoch 2, Loss: 0.9095159769058228\n",
            "Epoch 2, Loss: 1.0087875127792358\n",
            "Epoch 2, Loss: 0.9157652854919434\n",
            "Epoch 2, Loss: 0.8507556915283203\n",
            "Epoch 2, Loss: 0.8792324066162109\n",
            "Epoch 2, Loss: 0.8963860273361206\n",
            "Epoch 2, Loss: 0.8786924481391907\n",
            "Epoch 2, Loss: 0.9929831027984619\n",
            "Epoch 2, Loss: 0.8768656253814697\n",
            "Epoch 2, Loss: 0.9491270780563354\n",
            "Epoch 2, Loss: 0.8502303957939148\n",
            "Epoch 2, Loss: 0.8732010126113892\n",
            "Epoch 2, Loss: 1.0986253023147583\n",
            "Epoch 2, Loss: 0.9730551242828369\n",
            "Epoch 2, Loss: 0.8939946889877319\n",
            "Epoch 2, Loss: 0.9039385914802551\n",
            "Epoch 2, Loss: 0.8938279151916504\n",
            "Epoch 2, Loss: 0.8832333087921143\n",
            "Epoch 2, Loss: 0.9037759900093079\n",
            "Epoch 2, Loss: 0.8730633854866028\n",
            "Epoch 2, Loss: 0.9302207231521606\n",
            "Epoch 2, Loss: 0.9310159087181091\n",
            "Epoch 2, Loss: 0.8635333776473999\n",
            "Epoch 2, Loss: 0.9428236484527588\n",
            "Epoch 2, Loss: 0.9411085844039917\n",
            "Epoch 2, Loss: 0.9402806758880615\n",
            "Epoch 2, Loss: 0.9077625274658203\n",
            "Epoch 2, Loss: 0.8666544556617737\n",
            "Epoch 2, Loss: 0.9915854930877686\n",
            "Epoch 2, Loss: 0.9027320742607117\n",
            "Epoch 2, Loss: 1.060221552848816\n",
            "Epoch 2, Loss: 0.9267339706420898\n",
            "Epoch 2, Loss: 0.8610087633132935\n",
            "Epoch 2, Loss: 0.8511850237846375\n",
            "Epoch 2, Loss: 0.9393302202224731\n",
            "Epoch 2, Loss: 0.8785226941108704\n",
            "Epoch 2, Loss: 1.0679984092712402\n",
            "Epoch 2, Loss: 0.871868908405304\n",
            "Epoch 2, Loss: 0.9145520329475403\n",
            "Epoch 2, Loss: 0.9020721912384033\n",
            "Epoch 2, Loss: 0.9301326274871826\n",
            "Epoch 2, Loss: 0.8954747915267944\n",
            "Epoch 2, Loss: 0.9165642261505127\n",
            "Epoch 2, Loss: 0.8986803293228149\n",
            "Epoch 2, Loss: 0.857904314994812\n",
            "Epoch 2, Loss: 0.8731764554977417\n",
            "Epoch 2, Loss: 0.9633140563964844\n",
            "Epoch 2, Loss: 0.8793339133262634\n",
            "Epoch 2, Loss: 0.8541580438613892\n",
            "Epoch 2, Loss: 0.931587278842926\n",
            "Epoch 2, Loss: 0.9267957806587219\n",
            "Epoch 2, Loss: 0.8899589776992798\n",
            "Epoch 2, Loss: 0.9390517473220825\n",
            "Epoch 2, Loss: 0.9185252785682678\n",
            "Epoch 2, Loss: 0.9126969575881958\n",
            "Epoch 2, Loss: 0.8605512976646423\n",
            "Epoch 2, Loss: 0.8612701296806335\n",
            "Epoch 2, Loss: 1.0449020862579346\n",
            "Epoch 2, Loss: 0.8510900735855103\n",
            "Epoch 2, Loss: 0.9048351049423218\n",
            "Epoch 2, Loss: 0.9423457384109497\n",
            "Epoch 2, Loss: 1.0207228660583496\n",
            "Epoch 2, Loss: 0.8592839241027832\n",
            "Epoch 2, Loss: 0.9470838904380798\n",
            "Epoch 2, Loss: 0.9195597171783447\n",
            "Epoch 2, Loss: 0.8564713001251221\n",
            "Epoch 2, Loss: 0.8853312730789185\n",
            "Epoch 2, Loss: 0.9145247936248779\n",
            "Epoch 2, Loss: 0.9049087762832642\n",
            "Epoch 2, Loss: 0.9030754566192627\n",
            "Epoch 2, Loss: 0.9037715196609497\n",
            "Epoch 2, Loss: 0.9174618124961853\n",
            "Epoch 2, Loss: 0.9828001260757446\n",
            "Epoch 2, Loss: 0.9345185160636902\n",
            "Epoch 2, Loss: 0.9294801354408264\n",
            "Epoch 2, Loss: 0.9301626682281494\n",
            "Epoch 2, Loss: 0.9324272871017456\n",
            "Epoch 2, Loss: 0.8890896439552307\n",
            "Epoch 2, Loss: 0.8424766659736633\n",
            "Epoch 2, Loss: 0.9070501327514648\n",
            "Epoch 2, Loss: 0.9708287715911865\n",
            "Epoch 3, Loss: 0.9321672320365906\n",
            "Epoch 3, Loss: 0.8787103891372681\n",
            "Epoch 3, Loss: 0.9436314105987549\n",
            "Epoch 3, Loss: 0.9458855986595154\n",
            "Epoch 3, Loss: 0.8746264576911926\n",
            "Epoch 3, Loss: 0.9064754843711853\n",
            "Epoch 3, Loss: 0.8731730580329895\n",
            "Epoch 3, Loss: 0.9283984899520874\n",
            "Epoch 3, Loss: 0.9193402528762817\n",
            "Epoch 3, Loss: 0.9142531156539917\n",
            "Epoch 3, Loss: 0.9203653335571289\n",
            "Epoch 3, Loss: 1.0631532669067383\n",
            "Epoch 3, Loss: 0.8649696707725525\n",
            "Epoch 3, Loss: 0.8852354884147644\n",
            "Epoch 3, Loss: 1.080044150352478\n",
            "Epoch 3, Loss: 0.938578724861145\n",
            "Epoch 3, Loss: 0.8786522150039673\n",
            "Epoch 3, Loss: 0.9476335644721985\n",
            "Epoch 3, Loss: 1.0056335926055908\n",
            "Epoch 3, Loss: 0.9515495896339417\n",
            "Epoch 3, Loss: 0.8577210903167725\n",
            "Epoch 3, Loss: 0.8795313835144043\n",
            "Epoch 3, Loss: 0.891313374042511\n",
            "Epoch 3, Loss: 1.0718098878860474\n",
            "Epoch 3, Loss: 0.8817047476768494\n",
            "Epoch 3, Loss: 0.9197551012039185\n",
            "Epoch 3, Loss: 0.8818244338035583\n",
            "Epoch 3, Loss: 0.9056897759437561\n",
            "Epoch 3, Loss: 0.9737037420272827\n",
            "Epoch 3, Loss: 0.9980351328849792\n",
            "Epoch 3, Loss: 0.867279052734375\n",
            "Epoch 3, Loss: 0.8891674876213074\n",
            "Epoch 3, Loss: 0.9392979145050049\n",
            "Epoch 3, Loss: 1.0756794214248657\n",
            "Epoch 3, Loss: 0.9412312507629395\n",
            "Epoch 3, Loss: 0.902645468711853\n",
            "Epoch 3, Loss: 0.976681113243103\n",
            "Epoch 3, Loss: 0.9229367971420288\n",
            "Epoch 3, Loss: 0.9440984725952148\n",
            "Epoch 3, Loss: 0.9106146693229675\n",
            "Epoch 3, Loss: 0.8613601326942444\n",
            "Epoch 3, Loss: 0.9799259305000305\n",
            "Epoch 3, Loss: 0.8700884580612183\n",
            "Epoch 3, Loss: 0.8841265439987183\n",
            "Epoch 3, Loss: 0.9219385385513306\n",
            "Epoch 3, Loss: 0.9288853406906128\n",
            "Epoch 3, Loss: 0.8935286402702332\n",
            "Epoch 3, Loss: 0.8772521615028381\n",
            "Epoch 3, Loss: 0.863103985786438\n",
            "Epoch 3, Loss: 1.0402237176895142\n",
            "Epoch 3, Loss: 0.9010506868362427\n",
            "Epoch 3, Loss: 0.9833602905273438\n",
            "Epoch 3, Loss: 0.9736714363098145\n",
            "Epoch 3, Loss: 0.903506875038147\n",
            "Epoch 3, Loss: 1.0186636447906494\n",
            "Epoch 3, Loss: 0.9026256203651428\n",
            "Epoch 3, Loss: 0.9195696115493774\n",
            "Epoch 3, Loss: 1.024021029472351\n",
            "Epoch 3, Loss: 0.8811575770378113\n",
            "Epoch 3, Loss: 0.8669130802154541\n",
            "Epoch 3, Loss: 0.9160404801368713\n",
            "Epoch 3, Loss: 0.8944675326347351\n",
            "Epoch 3, Loss: 0.8973938226699829\n",
            "Epoch 3, Loss: 0.8571692705154419\n",
            "Epoch 3, Loss: 0.9151308536529541\n",
            "Epoch 3, Loss: 0.9106127619743347\n",
            "Epoch 3, Loss: 0.870071530342102\n",
            "Epoch 3, Loss: 0.8598998785018921\n",
            "Epoch 3, Loss: 0.9769045114517212\n",
            "Epoch 3, Loss: 0.9025579690933228\n",
            "Epoch 3, Loss: 0.8773091435432434\n",
            "Epoch 3, Loss: 0.9175357222557068\n",
            "Epoch 3, Loss: 0.8794345259666443\n",
            "Epoch 3, Loss: 0.8756371736526489\n",
            "Epoch 3, Loss: 0.950495183467865\n",
            "Epoch 3, Loss: 0.8578684329986572\n",
            "Epoch 3, Loss: 0.9246313571929932\n",
            "Epoch 3, Loss: 0.9384400844573975\n",
            "Epoch 3, Loss: 0.899127721786499\n",
            "Epoch 3, Loss: 0.8859385251998901\n",
            "Epoch 3, Loss: 0.8886562585830688\n",
            "Epoch 3, Loss: 0.923831045627594\n",
            "Epoch 3, Loss: 0.8818276524543762\n",
            "Epoch 3, Loss: 0.9831136465072632\n",
            "Epoch 3, Loss: 0.9642167091369629\n",
            "Epoch 3, Loss: 0.9236406087875366\n",
            "Epoch 3, Loss: 0.8970533013343811\n",
            "Epoch 3, Loss: 0.8886056542396545\n",
            "Epoch 3, Loss: 0.9572011232376099\n",
            "Epoch 3, Loss: 0.9469032883644104\n",
            "Epoch 3, Loss: 1.1922500133514404\n",
            "Epoch 3, Loss: 0.9310277104377747\n",
            "Epoch 3, Loss: 0.855181097984314\n",
            "Epoch 3, Loss: 0.8688423037528992\n",
            "Epoch 3, Loss: 0.8659713268280029\n",
            "Epoch 3, Loss: 0.9385647177696228\n",
            "Epoch 3, Loss: 0.8371801376342773\n",
            "Epoch 3, Loss: 0.8726934194564819\n",
            "Epoch 3, Loss: 0.9267624616622925\n",
            "Epoch 3, Loss: 0.8525063991546631\n",
            "Epoch 3, Loss: 0.9173734188079834\n",
            "Epoch 3, Loss: 0.8972465395927429\n",
            "Epoch 3, Loss: 0.8685784935951233\n",
            "Epoch 3, Loss: 0.9735490083694458\n",
            "Epoch 3, Loss: 0.9446021318435669\n",
            "Epoch 3, Loss: 0.8955197334289551\n",
            "Epoch 3, Loss: 1.0040900707244873\n",
            "Epoch 3, Loss: 0.912627637386322\n",
            "Epoch 3, Loss: 0.9460183382034302\n",
            "Epoch 3, Loss: 0.8812955021858215\n",
            "Epoch 3, Loss: 1.0611984729766846\n",
            "Epoch 3, Loss: 0.904299259185791\n",
            "Epoch 3, Loss: 0.9065057635307312\n",
            "Epoch 3, Loss: 0.9532947540283203\n",
            "Epoch 3, Loss: 0.9478335976600647\n",
            "Epoch 3, Loss: 0.8859571218490601\n",
            "Epoch 3, Loss: 0.90264892578125\n",
            "Epoch 3, Loss: 0.9356448650360107\n",
            "Epoch 3, Loss: 1.0378997325897217\n",
            "Epoch 3, Loss: 0.895778238773346\n",
            "Epoch 3, Loss: 0.8853680491447449\n",
            "Epoch 3, Loss: 0.9890877604484558\n",
            "Epoch 3, Loss: 0.8728073835372925\n",
            "Epoch 3, Loss: 0.8943641781806946\n",
            "Epoch 3, Loss: 0.9378122091293335\n",
            "Epoch 3, Loss: 1.2228273153305054\n",
            "Epoch 3, Loss: 0.8978723883628845\n",
            "Epoch 3, Loss: 0.9052790999412537\n",
            "Epoch 3, Loss: 1.057023525238037\n",
            "Epoch 3, Loss: 0.8937245011329651\n",
            "Epoch 3, Loss: 0.9988085627555847\n",
            "Epoch 3, Loss: 0.9932137131690979\n",
            "Epoch 3, Loss: 0.9084753394126892\n",
            "Epoch 3, Loss: 0.8917250037193298\n",
            "Epoch 3, Loss: 0.895290732383728\n",
            "Epoch 3, Loss: 0.9341980814933777\n",
            "Epoch 3, Loss: 0.862163245677948\n",
            "Epoch 3, Loss: 0.9139100313186646\n",
            "Epoch 3, Loss: 0.8627078533172607\n",
            "Epoch 3, Loss: 0.8986391425132751\n",
            "Epoch 3, Loss: 0.8511637449264526\n",
            "Epoch 3, Loss: 0.9227681159973145\n",
            "Epoch 3, Loss: 1.0664352178573608\n",
            "Epoch 3, Loss: 0.9021235108375549\n",
            "Epoch 3, Loss: 0.8527425527572632\n",
            "Epoch 3, Loss: 0.9046961069107056\n",
            "Epoch 3, Loss: 0.8934800028800964\n",
            "Epoch 3, Loss: 0.9012582302093506\n",
            "Epoch 3, Loss: 0.9361954927444458\n",
            "Epoch 3, Loss: 0.8961958885192871\n",
            "Epoch 3, Loss: 0.8779430985450745\n",
            "Epoch 3, Loss: 0.9234082102775574\n",
            "Epoch 3, Loss: 0.9034814834594727\n",
            "Epoch 3, Loss: 0.8702378273010254\n",
            "Epoch 3, Loss: 0.951513946056366\n",
            "Epoch 3, Loss: 0.927927553653717\n",
            "Epoch 3, Loss: 0.8719338774681091\n",
            "Epoch 3, Loss: 0.9154223203659058\n",
            "Epoch 3, Loss: 0.9090097546577454\n",
            "Epoch 3, Loss: 0.996520459651947\n",
            "Epoch 3, Loss: 0.8524706959724426\n",
            "Epoch 3, Loss: 0.970483124256134\n",
            "Epoch 3, Loss: 0.8672672510147095\n",
            "Epoch 3, Loss: 0.9763777852058411\n",
            "Epoch 3, Loss: 1.0446261167526245\n",
            "Epoch 3, Loss: 0.8802257180213928\n",
            "Epoch 3, Loss: 0.9215361475944519\n",
            "Epoch 3, Loss: 0.8796240091323853\n",
            "Epoch 3, Loss: 0.9634198546409607\n",
            "Epoch 3, Loss: 0.9598835110664368\n",
            "Epoch 3, Loss: 0.9878314137458801\n",
            "Epoch 3, Loss: 0.8721188902854919\n",
            "Epoch 3, Loss: 1.0189788341522217\n",
            "Epoch 3, Loss: 0.9064008593559265\n",
            "Epoch 3, Loss: 0.8709138035774231\n",
            "Epoch 3, Loss: 0.9166097044944763\n",
            "Epoch 4, Loss: 0.920971155166626\n",
            "Epoch 4, Loss: 0.9629892110824585\n",
            "Epoch 4, Loss: 0.9115004539489746\n",
            "Epoch 4, Loss: 0.8878908753395081\n",
            "Epoch 4, Loss: 0.8737496733665466\n",
            "Epoch 4, Loss: 1.178054690361023\n",
            "Epoch 4, Loss: 0.904462993144989\n",
            "Epoch 4, Loss: 0.8736222982406616\n",
            "Epoch 4, Loss: 0.8396523594856262\n",
            "Epoch 4, Loss: 0.8719668984413147\n",
            "Epoch 4, Loss: 0.8918082118034363\n",
            "Epoch 4, Loss: 0.9107034802436829\n",
            "Epoch 4, Loss: 0.9512300491333008\n",
            "Epoch 4, Loss: 0.89840167760849\n",
            "Epoch 4, Loss: 0.8962520360946655\n",
            "Epoch 4, Loss: 0.9157297015190125\n",
            "Epoch 4, Loss: 0.9732294082641602\n",
            "Epoch 4, Loss: 1.0164766311645508\n",
            "Epoch 4, Loss: 0.9215983748435974\n",
            "Epoch 4, Loss: 0.9090093970298767\n",
            "Epoch 4, Loss: 0.8858675956726074\n",
            "Epoch 4, Loss: 0.9479383826255798\n",
            "Epoch 4, Loss: 0.8823339343070984\n",
            "Epoch 4, Loss: 0.8923172950744629\n",
            "Epoch 4, Loss: 0.8904486298561096\n",
            "Epoch 4, Loss: 0.9474672079086304\n",
            "Epoch 4, Loss: 1.012149453163147\n",
            "Epoch 4, Loss: 0.833976149559021\n",
            "Epoch 4, Loss: 0.857056200504303\n",
            "Epoch 4, Loss: 0.8902673721313477\n",
            "Epoch 4, Loss: 0.9507246613502502\n",
            "Epoch 4, Loss: 1.0656503438949585\n",
            "Epoch 4, Loss: 0.8904650211334229\n",
            "Epoch 4, Loss: 0.9487674832344055\n",
            "Epoch 4, Loss: 0.9051660299301147\n",
            "Epoch 4, Loss: 0.923721432685852\n",
            "Epoch 4, Loss: 1.0582683086395264\n",
            "Epoch 4, Loss: 0.8945609927177429\n",
            "Epoch 4, Loss: 0.9718586802482605\n",
            "Epoch 4, Loss: 0.9015174508094788\n",
            "Epoch 4, Loss: 0.907541811466217\n",
            "Epoch 4, Loss: 0.9047923684120178\n",
            "Epoch 4, Loss: 0.9142587184906006\n",
            "Epoch 4, Loss: 0.8889756202697754\n",
            "Epoch 4, Loss: 0.9232020974159241\n",
            "Epoch 4, Loss: 0.8704431056976318\n",
            "Epoch 4, Loss: 0.9165237545967102\n",
            "Epoch 4, Loss: 0.9715033173561096\n",
            "Epoch 4, Loss: 0.8480068445205688\n",
            "Epoch 4, Loss: 0.9253095984458923\n",
            "Epoch 4, Loss: 0.9329226016998291\n",
            "Epoch 4, Loss: 0.9287927746772766\n",
            "Epoch 4, Loss: 0.9105567336082458\n",
            "Epoch 4, Loss: 0.9025976657867432\n",
            "Epoch 4, Loss: 0.9321684241294861\n",
            "Epoch 4, Loss: 0.8755888938903809\n",
            "Epoch 4, Loss: 0.926963746547699\n",
            "Epoch 4, Loss: 0.9776568412780762\n",
            "Epoch 4, Loss: 1.0825724601745605\n",
            "Epoch 4, Loss: 0.8878263831138611\n",
            "Epoch 4, Loss: 0.8707326650619507\n",
            "Epoch 4, Loss: 1.086112380027771\n",
            "Epoch 4, Loss: 1.031476378440857\n",
            "Epoch 4, Loss: 0.8957313895225525\n",
            "Epoch 4, Loss: 0.9048821926116943\n",
            "Epoch 4, Loss: 0.8933936953544617\n",
            "Epoch 4, Loss: 1.0404061079025269\n",
            "Epoch 4, Loss: 0.9447394609451294\n",
            "Epoch 4, Loss: 0.9181311726570129\n",
            "Epoch 4, Loss: 1.155124545097351\n",
            "Epoch 4, Loss: 0.9543472528457642\n",
            "Epoch 4, Loss: 0.9371211528778076\n",
            "Epoch 4, Loss: 0.9094352126121521\n",
            "Epoch 4, Loss: 0.8827004432678223\n",
            "Epoch 4, Loss: 0.9469119310379028\n",
            "Epoch 4, Loss: 0.860731303691864\n",
            "Epoch 4, Loss: 0.9268605709075928\n",
            "Epoch 4, Loss: 0.8686143755912781\n",
            "Epoch 4, Loss: 0.87538081407547\n",
            "Epoch 4, Loss: 0.8295872211456299\n",
            "Epoch 4, Loss: 0.8827271461486816\n",
            "Epoch 4, Loss: 0.9750394821166992\n",
            "Epoch 4, Loss: 0.8984489440917969\n",
            "Epoch 4, Loss: 0.911571204662323\n",
            "Epoch 4, Loss: 0.9178901314735413\n",
            "Epoch 4, Loss: 0.8628880381584167\n",
            "Epoch 4, Loss: 1.064802646636963\n",
            "Epoch 4, Loss: 0.8905946016311646\n",
            "Epoch 4, Loss: 0.8792403340339661\n",
            "Epoch 4, Loss: 0.8892936110496521\n",
            "Epoch 4, Loss: 0.9304330348968506\n",
            "Epoch 4, Loss: 0.9023644328117371\n",
            "Epoch 4, Loss: 0.8954805731773376\n",
            "Epoch 4, Loss: 0.9587770700454712\n",
            "Epoch 4, Loss: 0.9362455606460571\n",
            "Epoch 4, Loss: 0.9359318614006042\n",
            "Epoch 4, Loss: 0.8817671537399292\n",
            "Epoch 4, Loss: 0.8896889090538025\n",
            "Epoch 4, Loss: 1.0512206554412842\n",
            "Epoch 4, Loss: 0.8811683654785156\n",
            "Epoch 4, Loss: 0.9190033078193665\n",
            "Epoch 4, Loss: 1.0458325147628784\n",
            "Epoch 4, Loss: 0.9243191480636597\n",
            "Epoch 4, Loss: 0.9342063665390015\n",
            "Epoch 4, Loss: 0.8323348760604858\n",
            "Epoch 4, Loss: 0.9600679278373718\n",
            "Epoch 4, Loss: 0.878330409526825\n",
            "Epoch 4, Loss: 0.9131199717521667\n",
            "Epoch 4, Loss: 0.8920406103134155\n",
            "Epoch 4, Loss: 0.8815284371376038\n",
            "Epoch 4, Loss: 0.8785882592201233\n",
            "Epoch 4, Loss: 0.9008558392524719\n",
            "Epoch 4, Loss: 0.8819103240966797\n",
            "Epoch 4, Loss: 0.8949906826019287\n",
            "Epoch 4, Loss: 0.9127885699272156\n",
            "Epoch 4, Loss: 0.8915905356407166\n",
            "Epoch 4, Loss: 0.8820789456367493\n",
            "Epoch 4, Loss: 0.9418412446975708\n",
            "Epoch 4, Loss: 0.8424477577209473\n",
            "Epoch 4, Loss: 0.9081652164459229\n",
            "Epoch 4, Loss: 0.8951156735420227\n",
            "Epoch 4, Loss: 1.0944563150405884\n",
            "Epoch 4, Loss: 0.9327700734138489\n",
            "Epoch 4, Loss: 0.9351867437362671\n",
            "Epoch 4, Loss: 0.9441561698913574\n",
            "Epoch 4, Loss: 0.945412814617157\n",
            "Epoch 4, Loss: 0.8811482787132263\n",
            "Epoch 4, Loss: 0.9410451650619507\n",
            "Epoch 4, Loss: 0.8933491110801697\n",
            "Epoch 4, Loss: 0.8855541348457336\n",
            "Epoch 4, Loss: 0.9026286602020264\n",
            "Epoch 4, Loss: 0.9483214616775513\n",
            "Epoch 4, Loss: 0.8958109021186829\n",
            "Epoch 4, Loss: 0.9204029440879822\n",
            "Epoch 4, Loss: 0.8854044675827026\n",
            "Epoch 4, Loss: 0.9568873047828674\n",
            "Epoch 4, Loss: 0.9087252020835876\n",
            "Epoch 4, Loss: 0.9196987748146057\n",
            "Epoch 4, Loss: 1.0007015466690063\n",
            "Epoch 4, Loss: 0.938988208770752\n",
            "Epoch 4, Loss: 0.9265039563179016\n",
            "Epoch 4, Loss: 0.858494758605957\n",
            "Epoch 4, Loss: 0.8500725626945496\n",
            "Epoch 4, Loss: 0.9037197828292847\n",
            "Epoch 4, Loss: 0.9519416689872742\n",
            "Epoch 4, Loss: 1.0454421043395996\n",
            "Epoch 4, Loss: 0.9808424115180969\n",
            "Epoch 4, Loss: 0.9027819633483887\n",
            "Epoch 4, Loss: 0.9203550219535828\n",
            "Epoch 4, Loss: 0.8896266222000122\n",
            "Epoch 4, Loss: 0.8966629505157471\n",
            "Epoch 4, Loss: 0.9875854849815369\n",
            "Epoch 4, Loss: 0.9567697644233704\n",
            "Epoch 4, Loss: 0.9309788346290588\n",
            "Epoch 4, Loss: 0.9480141997337341\n",
            "Epoch 4, Loss: 0.9189088940620422\n",
            "Epoch 4, Loss: 0.87519770860672\n",
            "Epoch 4, Loss: 1.0031538009643555\n",
            "Epoch 4, Loss: 0.9059783816337585\n",
            "Epoch 4, Loss: 0.8723313212394714\n",
            "Epoch 4, Loss: 0.8944677710533142\n",
            "Epoch 4, Loss: 0.8822683095932007\n",
            "Epoch 4, Loss: 0.8637519478797913\n",
            "Epoch 4, Loss: 0.8716915249824524\n",
            "Epoch 4, Loss: 1.0217056274414062\n",
            "Epoch 4, Loss: 1.1253844499588013\n",
            "Epoch 4, Loss: 0.8539690375328064\n",
            "Epoch 4, Loss: 0.9461460113525391\n",
            "Epoch 4, Loss: 0.9010722637176514\n",
            "Epoch 4, Loss: 0.952671468257904\n",
            "Epoch 4, Loss: 0.9824692606925964\n",
            "Epoch 4, Loss: 0.9939241409301758\n",
            "Epoch 4, Loss: 0.8618646264076233\n",
            "Epoch 4, Loss: 0.884651780128479\n",
            "Epoch 4, Loss: 0.8691686987876892\n",
            "Epoch 4, Loss: 0.9180588722229004\n",
            "Epoch 5, Loss: 0.9137969017028809\n",
            "Epoch 5, Loss: 0.8691826462745667\n",
            "Epoch 5, Loss: 0.9679274559020996\n",
            "Epoch 5, Loss: 0.8562237620353699\n",
            "Epoch 5, Loss: 0.8845469355583191\n",
            "Epoch 5, Loss: 0.8813282251358032\n",
            "Epoch 5, Loss: 0.8943955302238464\n",
            "Epoch 5, Loss: 0.9417004585266113\n",
            "Epoch 5, Loss: 0.886332631111145\n",
            "Epoch 5, Loss: 0.8950430154800415\n",
            "Epoch 5, Loss: 0.8867048621177673\n",
            "Epoch 5, Loss: 1.097248911857605\n",
            "Epoch 5, Loss: 0.884275496006012\n",
            "Epoch 5, Loss: 0.956030547618866\n",
            "Epoch 5, Loss: 0.9107483625411987\n",
            "Epoch 5, Loss: 0.9270632266998291\n",
            "Epoch 5, Loss: 0.8833970427513123\n",
            "Epoch 5, Loss: 0.9586842060089111\n",
            "Epoch 5, Loss: 0.914518415927887\n",
            "Epoch 5, Loss: 0.9526709914207458\n",
            "Epoch 5, Loss: 0.8608877658843994\n",
            "Epoch 5, Loss: 0.9374446272850037\n",
            "Epoch 5, Loss: 1.0037956237792969\n",
            "Epoch 5, Loss: 0.9361127614974976\n",
            "Epoch 5, Loss: 0.9896223545074463\n",
            "Epoch 5, Loss: 0.944546639919281\n",
            "Epoch 5, Loss: 0.9933804869651794\n",
            "Epoch 5, Loss: 1.0036176443099976\n",
            "Epoch 5, Loss: 0.8802507519721985\n",
            "Epoch 5, Loss: 0.9110627770423889\n",
            "Epoch 5, Loss: 0.9102969765663147\n",
            "Epoch 5, Loss: 0.8776083588600159\n",
            "Epoch 5, Loss: 0.8701438307762146\n",
            "Epoch 5, Loss: 0.9203336834907532\n",
            "Epoch 5, Loss: 0.9934965372085571\n",
            "Epoch 5, Loss: 0.9010156393051147\n",
            "Epoch 5, Loss: 1.000861406326294\n",
            "Epoch 5, Loss: 0.9786274433135986\n",
            "Epoch 5, Loss: 0.8890089988708496\n",
            "Epoch 5, Loss: 1.0675690174102783\n",
            "Epoch 5, Loss: 0.9235221743583679\n",
            "Epoch 5, Loss: 0.865993082523346\n",
            "Epoch 5, Loss: 1.001530408859253\n",
            "Epoch 5, Loss: 0.9149498343467712\n",
            "Epoch 5, Loss: 0.9537064433097839\n",
            "Epoch 5, Loss: 0.9411700367927551\n",
            "Epoch 5, Loss: 0.8833757042884827\n",
            "Epoch 5, Loss: 0.9932079315185547\n",
            "Epoch 5, Loss: 0.855765700340271\n",
            "Epoch 5, Loss: 0.8702898025512695\n",
            "Epoch 5, Loss: 0.9348542094230652\n",
            "Epoch 5, Loss: 0.8767338395118713\n",
            "Epoch 5, Loss: 0.9478044509887695\n",
            "Epoch 5, Loss: 0.8872867226600647\n",
            "Epoch 5, Loss: 0.8743024468421936\n",
            "Epoch 5, Loss: 0.9064884185791016\n",
            "Epoch 5, Loss: 0.9107060432434082\n",
            "Epoch 5, Loss: 0.883912205696106\n",
            "Epoch 5, Loss: 0.9336262345314026\n",
            "Epoch 5, Loss: 0.9388089179992676\n",
            "Epoch 5, Loss: 0.9887863397598267\n",
            "Epoch 5, Loss: 0.868375837802887\n",
            "Epoch 5, Loss: 0.8872843384742737\n",
            "Epoch 5, Loss: 0.990157425403595\n",
            "Epoch 5, Loss: 0.8713584542274475\n",
            "Epoch 5, Loss: 1.0225780010223389\n",
            "Epoch 5, Loss: 0.8970348834991455\n",
            "Epoch 5, Loss: 1.1229667663574219\n",
            "Epoch 5, Loss: 0.9247633218765259\n",
            "Epoch 5, Loss: 0.8947861790657043\n",
            "Epoch 5, Loss: 0.8907250761985779\n",
            "Epoch 5, Loss: 0.9272868633270264\n",
            "Epoch 5, Loss: 0.9122152328491211\n",
            "Epoch 5, Loss: 0.8950110077857971\n",
            "Epoch 5, Loss: 0.8825921416282654\n",
            "Epoch 5, Loss: 0.8680582642555237\n",
            "Epoch 5, Loss: 0.8651857972145081\n",
            "Epoch 5, Loss: 0.9076526761054993\n",
            "Epoch 5, Loss: 0.9086036086082458\n",
            "Epoch 5, Loss: 0.9046024084091187\n",
            "Epoch 5, Loss: 0.8690598607063293\n",
            "Epoch 5, Loss: 0.9008158445358276\n",
            "Epoch 5, Loss: 0.9554017186164856\n",
            "Epoch 5, Loss: 0.8869622349739075\n",
            "Epoch 5, Loss: 0.9384740591049194\n",
            "Epoch 5, Loss: 1.0268903970718384\n",
            "Epoch 5, Loss: 0.8830138444900513\n",
            "Epoch 5, Loss: 1.072178602218628\n",
            "Epoch 5, Loss: 1.0673160552978516\n",
            "Epoch 5, Loss: 0.8655160665512085\n",
            "Epoch 5, Loss: 0.9762061238288879\n",
            "Epoch 5, Loss: 0.9196057915687561\n",
            "Epoch 5, Loss: 0.8940817713737488\n",
            "Epoch 5, Loss: 1.001854658126831\n",
            "Epoch 5, Loss: 0.8823673725128174\n",
            "Epoch 5, Loss: 0.8375563621520996\n",
            "Epoch 5, Loss: 0.8902988433837891\n",
            "Epoch 5, Loss: 0.8528236150741577\n",
            "Epoch 5, Loss: 0.8790964484214783\n",
            "Epoch 5, Loss: 0.8768952488899231\n",
            "Epoch 5, Loss: 0.8934714198112488\n",
            "Epoch 5, Loss: 0.9265571236610413\n",
            "Epoch 5, Loss: 0.8986859917640686\n",
            "Epoch 5, Loss: 0.8835896253585815\n",
            "Epoch 5, Loss: 0.863693356513977\n",
            "Epoch 5, Loss: 0.9890046715736389\n",
            "Epoch 5, Loss: 1.0342726707458496\n",
            "Epoch 5, Loss: 0.9407715201377869\n",
            "Epoch 5, Loss: 0.9080410599708557\n",
            "Epoch 5, Loss: 0.9042980670928955\n",
            "Epoch 5, Loss: 0.9435282349586487\n",
            "Epoch 5, Loss: 0.9916095733642578\n",
            "Epoch 5, Loss: 0.9909749031066895\n",
            "Epoch 5, Loss: 0.9210982918739319\n",
            "Epoch 5, Loss: 0.8463419675827026\n",
            "Epoch 5, Loss: 0.9086841940879822\n",
            "Epoch 5, Loss: 0.8761541247367859\n",
            "Epoch 5, Loss: 0.8808200359344482\n",
            "Epoch 5, Loss: 1.1022573709487915\n",
            "Epoch 5, Loss: 0.9232068061828613\n",
            "Epoch 5, Loss: 0.9397291541099548\n",
            "Epoch 5, Loss: 1.0383645296096802\n",
            "Epoch 5, Loss: 0.8685767650604248\n",
            "Epoch 5, Loss: 0.9024266004562378\n",
            "Epoch 5, Loss: 0.9043674468994141\n",
            "Epoch 5, Loss: 0.8803422451019287\n",
            "Epoch 5, Loss: 0.9409707188606262\n",
            "Epoch 5, Loss: 0.8655288219451904\n",
            "Epoch 5, Loss: 0.8966074585914612\n",
            "Epoch 5, Loss: 0.9576115012168884\n",
            "Epoch 5, Loss: 0.8547796010971069\n",
            "Epoch 5, Loss: 0.8798139691352844\n",
            "Epoch 5, Loss: 1.1808147430419922\n",
            "Epoch 5, Loss: 0.90082848072052\n",
            "Epoch 5, Loss: 0.8956778645515442\n",
            "Epoch 5, Loss: 0.9086960554122925\n",
            "Epoch 5, Loss: 0.8615452647209167\n",
            "Epoch 5, Loss: 1.0103394985198975\n",
            "Epoch 5, Loss: 0.9314369559288025\n",
            "Epoch 5, Loss: 0.8389947414398193\n",
            "Epoch 5, Loss: 0.9207258820533752\n",
            "Epoch 5, Loss: 0.9346266984939575\n",
            "Epoch 5, Loss: 0.9174460768699646\n",
            "Epoch 5, Loss: 1.1323763132095337\n",
            "Epoch 5, Loss: 0.980198323726654\n",
            "Epoch 5, Loss: 0.8913980722427368\n",
            "Epoch 5, Loss: 1.0535149574279785\n",
            "Epoch 5, Loss: 0.9209164381027222\n",
            "Epoch 5, Loss: 0.9687780141830444\n",
            "Epoch 5, Loss: 0.9320241808891296\n",
            "Epoch 5, Loss: 0.8541679978370667\n",
            "Epoch 5, Loss: 0.9901710152626038\n",
            "Epoch 5, Loss: 0.8605960607528687\n",
            "Epoch 5, Loss: 0.8747056722640991\n",
            "Epoch 5, Loss: 0.8932667970657349\n",
            "Epoch 5, Loss: 0.8538362383842468\n",
            "Epoch 5, Loss: 0.874254047870636\n",
            "Epoch 5, Loss: 0.9986192584037781\n",
            "Epoch 5, Loss: 0.9717110991477966\n",
            "Epoch 5, Loss: 0.9111210107803345\n",
            "Epoch 5, Loss: 0.9011422395706177\n",
            "Epoch 5, Loss: 0.9305093288421631\n",
            "Epoch 5, Loss: 0.8920080065727234\n",
            "Epoch 5, Loss: 0.8960251808166504\n",
            "Epoch 5, Loss: 1.0737597942352295\n",
            "Epoch 5, Loss: 0.9009862542152405\n",
            "Epoch 5, Loss: 0.9048919081687927\n",
            "Epoch 5, Loss: 0.8864198923110962\n",
            "Epoch 5, Loss: 0.8886926770210266\n",
            "Epoch 5, Loss: 0.8876564502716064\n",
            "Epoch 5, Loss: 0.9392832517623901\n",
            "Epoch 5, Loss: 0.9258416891098022\n",
            "Epoch 5, Loss: 0.8672564625740051\n",
            "Epoch 5, Loss: 0.8720914125442505\n",
            "Epoch 5, Loss: 0.9315744638442993\n",
            "Epoch 5, Loss: 0.9724414944648743\n",
            "Epoch 6, Loss: 0.8664318919181824\n",
            "Epoch 6, Loss: 0.8708928227424622\n",
            "Epoch 6, Loss: 0.9074469208717346\n",
            "Epoch 6, Loss: 0.8604558706283569\n",
            "Epoch 6, Loss: 0.9686734676361084\n",
            "Epoch 6, Loss: 0.8753751516342163\n",
            "Epoch 6, Loss: 0.8987305760383606\n",
            "Epoch 6, Loss: 0.9164556860923767\n",
            "Epoch 6, Loss: 0.8885202407836914\n",
            "Epoch 6, Loss: 0.8805481791496277\n",
            "Epoch 6, Loss: 1.0101491212844849\n",
            "Epoch 6, Loss: 0.9123971462249756\n",
            "Epoch 6, Loss: 0.9052668213844299\n",
            "Epoch 6, Loss: 0.9580855965614319\n",
            "Epoch 6, Loss: 0.8945914506912231\n",
            "Epoch 6, Loss: 0.892149806022644\n",
            "Epoch 6, Loss: 0.902634859085083\n",
            "Epoch 6, Loss: 0.9485678672790527\n",
            "Epoch 6, Loss: 0.8919969797134399\n",
            "Epoch 6, Loss: 0.8973463773727417\n",
            "Epoch 6, Loss: 0.8890668749809265\n",
            "Epoch 6, Loss: 0.9223520755767822\n",
            "Epoch 6, Loss: 0.9119774103164673\n",
            "Epoch 6, Loss: 0.9186424612998962\n",
            "Epoch 6, Loss: 0.8945788741111755\n",
            "Epoch 6, Loss: 0.9112204313278198\n",
            "Epoch 6, Loss: 0.9155586957931519\n",
            "Epoch 6, Loss: 0.8820027112960815\n",
            "Epoch 6, Loss: 0.907267689704895\n",
            "Epoch 6, Loss: 0.9546164870262146\n",
            "Epoch 6, Loss: 0.8620525002479553\n",
            "Epoch 6, Loss: 0.876013994216919\n",
            "Epoch 6, Loss: 0.9157972931861877\n",
            "Epoch 6, Loss: 0.9540687799453735\n",
            "Epoch 6, Loss: 0.9313549399375916\n",
            "Epoch 6, Loss: 0.8773825764656067\n",
            "Epoch 6, Loss: 0.8435415625572205\n",
            "Epoch 6, Loss: 0.8845088481903076\n",
            "Epoch 6, Loss: 0.9429863691329956\n",
            "Epoch 6, Loss: 0.9454434514045715\n",
            "Epoch 6, Loss: 0.8817467093467712\n",
            "Epoch 6, Loss: 0.8952071070671082\n",
            "Epoch 6, Loss: 0.9177846908569336\n",
            "Epoch 6, Loss: 0.9153400659561157\n",
            "Epoch 6, Loss: 0.9363027215003967\n",
            "Epoch 6, Loss: 0.9533612728118896\n",
            "Epoch 6, Loss: 0.8818221688270569\n",
            "Epoch 6, Loss: 0.9266989231109619\n",
            "Epoch 6, Loss: 0.8891254663467407\n",
            "Epoch 6, Loss: 0.8779628872871399\n",
            "Epoch 6, Loss: 0.9572655558586121\n",
            "Epoch 6, Loss: 0.8764948844909668\n",
            "Epoch 6, Loss: 0.9688296318054199\n",
            "Epoch 6, Loss: 0.8798595666885376\n",
            "Epoch 6, Loss: 0.9878720641136169\n",
            "Epoch 6, Loss: 0.8620092272758484\n",
            "Epoch 6, Loss: 0.8880999684333801\n",
            "Epoch 6, Loss: 1.0006096363067627\n",
            "Epoch 6, Loss: 0.8490579128265381\n",
            "Epoch 6, Loss: 0.8775097131729126\n",
            "Epoch 6, Loss: 0.8917688131332397\n",
            "Epoch 6, Loss: 0.8678921461105347\n",
            "Epoch 6, Loss: 0.8977871537208557\n",
            "Epoch 6, Loss: 0.9288865923881531\n",
            "Epoch 6, Loss: 0.8549916744232178\n",
            "Epoch 6, Loss: 0.9130279421806335\n",
            "Epoch 6, Loss: 0.9405812621116638\n",
            "Epoch 6, Loss: 0.887830913066864\n",
            "Epoch 6, Loss: 0.8800907731056213\n",
            "Epoch 6, Loss: 0.9087891578674316\n",
            "Epoch 6, Loss: 0.8591424226760864\n",
            "Epoch 6, Loss: 0.8723943829536438\n",
            "Epoch 6, Loss: 0.9950618147850037\n",
            "Epoch 6, Loss: 0.8974924087524414\n",
            "Epoch 6, Loss: 0.9505648612976074\n",
            "Epoch 6, Loss: 1.04949951171875\n",
            "Epoch 6, Loss: 0.8855753540992737\n",
            "Epoch 6, Loss: 0.8715826272964478\n",
            "Epoch 6, Loss: 0.8931167125701904\n",
            "Epoch 6, Loss: 0.9713708162307739\n",
            "Epoch 6, Loss: 0.9764205813407898\n",
            "Epoch 6, Loss: 0.8943636417388916\n",
            "Epoch 6, Loss: 0.9458428621292114\n",
            "Epoch 6, Loss: 0.8920241594314575\n",
            "Epoch 6, Loss: 1.0308971405029297\n",
            "Epoch 6, Loss: 0.9029947519302368\n",
            "Epoch 6, Loss: 1.002262830734253\n",
            "Epoch 6, Loss: 0.9056901335716248\n",
            "Epoch 6, Loss: 1.0006928443908691\n",
            "Epoch 6, Loss: 0.9487010836601257\n",
            "Epoch 6, Loss: 0.8829814791679382\n",
            "Epoch 6, Loss: 0.8955948352813721\n",
            "Epoch 6, Loss: 0.9102038145065308\n",
            "Epoch 6, Loss: 0.977545976638794\n",
            "Epoch 6, Loss: 0.9075968265533447\n",
            "Epoch 6, Loss: 0.910441517829895\n",
            "Epoch 6, Loss: 0.865960419178009\n",
            "Epoch 6, Loss: 1.0578118562698364\n",
            "Epoch 6, Loss: 0.9691239595413208\n",
            "Epoch 6, Loss: 1.0921474695205688\n",
            "Epoch 6, Loss: 0.9232370257377625\n",
            "Epoch 6, Loss: 0.8616243600845337\n",
            "Epoch 6, Loss: 0.8687278628349304\n",
            "Epoch 6, Loss: 0.927413821220398\n",
            "Epoch 6, Loss: 0.847013533115387\n",
            "Epoch 6, Loss: 0.988603949546814\n",
            "Epoch 6, Loss: 0.942094087600708\n",
            "Epoch 6, Loss: 0.8725211024284363\n",
            "Epoch 6, Loss: 0.9517550468444824\n",
            "Epoch 6, Loss: 0.9442651271820068\n",
            "Epoch 6, Loss: 1.0179234743118286\n",
            "Epoch 6, Loss: 0.9617729187011719\n",
            "Epoch 6, Loss: 0.9037371277809143\n",
            "Epoch 6, Loss: 0.8707765936851501\n",
            "Epoch 6, Loss: 1.1611841917037964\n",
            "Epoch 6, Loss: 0.9191677570343018\n",
            "Epoch 6, Loss: 0.8808401226997375\n",
            "Epoch 6, Loss: 0.9955306649208069\n",
            "Epoch 6, Loss: 0.8576912879943848\n",
            "Epoch 6, Loss: 1.0905498266220093\n",
            "Epoch 6, Loss: 0.9263851642608643\n",
            "Epoch 6, Loss: 0.936901867389679\n",
            "Epoch 6, Loss: 0.9225804209709167\n",
            "Epoch 6, Loss: 0.8795050382614136\n",
            "Epoch 6, Loss: 0.9822161793708801\n",
            "Epoch 6, Loss: 0.9321719408035278\n",
            "Epoch 6, Loss: 0.9106020927429199\n",
            "Epoch 6, Loss: 0.8887109756469727\n",
            "Epoch 6, Loss: 0.8744875192642212\n",
            "Epoch 6, Loss: 0.8738561272621155\n",
            "Epoch 6, Loss: 0.8784207701683044\n",
            "Epoch 6, Loss: 0.8730842471122742\n",
            "Epoch 6, Loss: 0.9422379732131958\n",
            "Epoch 6, Loss: 0.948249101638794\n",
            "Epoch 6, Loss: 0.8845863938331604\n",
            "Epoch 6, Loss: 1.0062493085861206\n",
            "Epoch 6, Loss: 0.9436302185058594\n",
            "Epoch 6, Loss: 0.9379172325134277\n",
            "Epoch 6, Loss: 0.9070417284965515\n",
            "Epoch 6, Loss: 0.8836538791656494\n",
            "Epoch 6, Loss: 0.8954278826713562\n",
            "Epoch 6, Loss: 0.8598232865333557\n",
            "Epoch 6, Loss: 0.880091667175293\n",
            "Epoch 6, Loss: 0.9362514019012451\n",
            "Epoch 6, Loss: 1.1172369718551636\n",
            "Epoch 6, Loss: 1.0171308517456055\n",
            "Epoch 6, Loss: 1.0014703273773193\n",
            "Epoch 6, Loss: 0.9810266494750977\n",
            "Epoch 6, Loss: 0.9200449585914612\n",
            "Epoch 6, Loss: 0.9748467206954956\n",
            "Epoch 6, Loss: 0.882865309715271\n",
            "Epoch 6, Loss: 0.8780267238616943\n",
            "Epoch 6, Loss: 0.9645988941192627\n",
            "Epoch 6, Loss: 0.860571563243866\n",
            "Epoch 6, Loss: 1.1117092370986938\n",
            "Epoch 6, Loss: 0.9333798885345459\n",
            "Epoch 6, Loss: 1.0992484092712402\n",
            "Epoch 6, Loss: 0.9548057317733765\n",
            "Epoch 6, Loss: 0.9004582166671753\n",
            "Epoch 6, Loss: 0.9267314076423645\n",
            "Epoch 6, Loss: 0.8301641345024109\n",
            "Epoch 6, Loss: 1.0072046518325806\n",
            "Epoch 6, Loss: 0.9255728125572205\n",
            "Epoch 6, Loss: 0.8915003538131714\n",
            "Epoch 6, Loss: 0.9365330338478088\n",
            "Epoch 6, Loss: 0.9494027495384216\n",
            "Epoch 6, Loss: 1.1066821813583374\n",
            "Epoch 6, Loss: 0.9438449144363403\n",
            "Epoch 6, Loss: 0.8153234720230103\n",
            "Epoch 6, Loss: 0.9017055034637451\n",
            "Epoch 6, Loss: 1.0550657510757446\n",
            "Epoch 6, Loss: 0.8918172121047974\n",
            "Epoch 6, Loss: 0.8759299516677856\n",
            "Epoch 6, Loss: 0.908004879951477\n",
            "Epoch 6, Loss: 1.1300292015075684\n",
            "Epoch 6, Loss: 1.085444688796997\n",
            "Epoch 7, Loss: 0.8968179225921631\n",
            "Epoch 7, Loss: 0.9046748280525208\n",
            "Epoch 7, Loss: 0.9381598234176636\n",
            "Epoch 7, Loss: 0.9151955842971802\n",
            "Epoch 7, Loss: 0.9944844245910645\n",
            "Epoch 7, Loss: 0.9334525465965271\n",
            "Epoch 7, Loss: 0.9868767261505127\n",
            "Epoch 7, Loss: 0.9095010757446289\n",
            "Epoch 7, Loss: 0.8632580637931824\n",
            "Epoch 7, Loss: 0.9176599383354187\n",
            "Epoch 7, Loss: 0.9491377472877502\n",
            "Epoch 7, Loss: 0.9458403587341309\n",
            "Epoch 7, Loss: 0.9189122319221497\n",
            "Epoch 7, Loss: 0.866425096988678\n",
            "Epoch 7, Loss: 0.9049871563911438\n",
            "Epoch 7, Loss: 0.9214594960212708\n",
            "Epoch 7, Loss: 0.9478336572647095\n",
            "Epoch 7, Loss: 0.9154474139213562\n",
            "Epoch 7, Loss: 0.941942036151886\n",
            "Epoch 7, Loss: 0.9532003402709961\n",
            "Epoch 7, Loss: 0.9339874386787415\n",
            "Epoch 7, Loss: 0.9024582505226135\n",
            "Epoch 7, Loss: 0.926195502281189\n",
            "Epoch 7, Loss: 1.1644539833068848\n",
            "Epoch 7, Loss: 0.9568544030189514\n",
            "Epoch 7, Loss: 0.9188976883888245\n",
            "Epoch 7, Loss: 0.9053592681884766\n",
            "Epoch 7, Loss: 0.8594832420349121\n",
            "Epoch 7, Loss: 0.924580991268158\n",
            "Epoch 7, Loss: 1.050089955329895\n",
            "Epoch 7, Loss: 0.8962292671203613\n",
            "Epoch 7, Loss: 0.8932294249534607\n",
            "Epoch 7, Loss: 0.9370394349098206\n",
            "Epoch 7, Loss: 0.9199471473693848\n",
            "Epoch 7, Loss: 0.9776096343994141\n",
            "Epoch 7, Loss: 1.054665207862854\n",
            "Epoch 7, Loss: 0.9366745352745056\n",
            "Epoch 7, Loss: 0.891169548034668\n",
            "Epoch 7, Loss: 0.9113278985023499\n",
            "Epoch 7, Loss: 0.891669511795044\n",
            "Epoch 7, Loss: 0.9208040237426758\n",
            "Epoch 7, Loss: 0.8881328701972961\n",
            "Epoch 7, Loss: 0.9019901156425476\n",
            "Epoch 7, Loss: 0.8684406876564026\n",
            "Epoch 7, Loss: 0.8515395522117615\n",
            "Epoch 7, Loss: 0.9383335113525391\n",
            "Epoch 7, Loss: 0.905671238899231\n",
            "Epoch 7, Loss: 0.9518871307373047\n",
            "Epoch 7, Loss: 0.9716986417770386\n",
            "Epoch 7, Loss: 0.9280219674110413\n",
            "Epoch 7, Loss: 0.8580573797225952\n",
            "Epoch 7, Loss: 0.9090338945388794\n",
            "Epoch 7, Loss: 0.8913335800170898\n",
            "Epoch 7, Loss: 0.9935227632522583\n",
            "Epoch 7, Loss: 0.9071229100227356\n",
            "Epoch 7, Loss: 0.9290843605995178\n",
            "Epoch 7, Loss: 0.9283884167671204\n",
            "Epoch 7, Loss: 0.8830246925354004\n",
            "Epoch 7, Loss: 0.9377096891403198\n",
            "Epoch 7, Loss: 1.024230718612671\n",
            "Epoch 7, Loss: 0.8718409538269043\n",
            "Epoch 7, Loss: 0.9006903767585754\n",
            "Epoch 7, Loss: 0.9028040766716003\n",
            "Epoch 7, Loss: 1.0103397369384766\n",
            "Epoch 7, Loss: 0.9633176922798157\n",
            "Epoch 7, Loss: 0.938171923160553\n",
            "Epoch 7, Loss: 0.8875359296798706\n",
            "Epoch 7, Loss: 0.8814271688461304\n",
            "Epoch 7, Loss: 0.9594278931617737\n",
            "Epoch 7, Loss: 0.9085530638694763\n",
            "Epoch 7, Loss: 0.8582221865653992\n",
            "Epoch 7, Loss: 0.9814353585243225\n",
            "Epoch 7, Loss: 0.8919979333877563\n",
            "Epoch 7, Loss: 0.9274296760559082\n",
            "Epoch 7, Loss: 0.9324994087219238\n",
            "Epoch 7, Loss: 0.8501768112182617\n",
            "Epoch 7, Loss: 0.8581522703170776\n",
            "Epoch 7, Loss: 0.9248573184013367\n",
            "Epoch 7, Loss: 0.8677999377250671\n",
            "Epoch 7, Loss: 0.8481196761131287\n",
            "Epoch 7, Loss: 0.9509052634239197\n",
            "Epoch 7, Loss: 0.88588947057724\n",
            "Epoch 7, Loss: 0.8695391416549683\n",
            "Epoch 7, Loss: 0.8448473215103149\n",
            "Epoch 7, Loss: 0.8637713193893433\n",
            "Epoch 7, Loss: 0.9433501958847046\n",
            "Epoch 7, Loss: 0.9186572432518005\n",
            "Epoch 7, Loss: 0.9154464602470398\n",
            "Epoch 7, Loss: 0.8427976369857788\n",
            "Epoch 7, Loss: 0.9299775958061218\n",
            "Epoch 7, Loss: 0.9094478487968445\n",
            "Epoch 7, Loss: 0.9119396209716797\n",
            "Epoch 7, Loss: 0.9026631712913513\n",
            "Epoch 7, Loss: 0.874634325504303\n",
            "Epoch 7, Loss: 1.0122610330581665\n",
            "Epoch 7, Loss: 0.8912662863731384\n",
            "Epoch 7, Loss: 0.9145086407661438\n",
            "Epoch 7, Loss: 0.873115599155426\n",
            "Epoch 7, Loss: 0.8870109915733337\n",
            "Epoch 7, Loss: 0.9103630185127258\n",
            "Epoch 7, Loss: 0.8809618353843689\n",
            "Epoch 7, Loss: 0.8883034586906433\n",
            "Epoch 7, Loss: 0.9824979305267334\n",
            "Epoch 7, Loss: 0.9934706091880798\n",
            "Epoch 7, Loss: 0.901776909828186\n",
            "Epoch 7, Loss: 0.9019373655319214\n",
            "Epoch 7, Loss: 1.1154392957687378\n",
            "Epoch 7, Loss: 0.8832277655601501\n",
            "Epoch 7, Loss: 1.0925896167755127\n",
            "Epoch 7, Loss: 0.8822973966598511\n",
            "Epoch 7, Loss: 0.9109960794448853\n",
            "Epoch 7, Loss: 0.878454864025116\n",
            "Epoch 7, Loss: 0.8783402442932129\n",
            "Epoch 7, Loss: 1.0376085042953491\n",
            "Epoch 7, Loss: 0.887422502040863\n",
            "Epoch 7, Loss: 0.8749058842658997\n",
            "Epoch 7, Loss: 0.8818712830543518\n",
            "Epoch 7, Loss: 0.979094386100769\n",
            "Epoch 7, Loss: 0.8554300665855408\n",
            "Epoch 7, Loss: 0.924182116985321\n",
            "Epoch 7, Loss: 0.8557261228561401\n",
            "Epoch 7, Loss: 0.8985409736633301\n",
            "Epoch 7, Loss: 0.8568928241729736\n",
            "Epoch 7, Loss: 1.052865743637085\n",
            "Epoch 7, Loss: 0.8921840786933899\n",
            "Epoch 7, Loss: 0.9040351510047913\n",
            "Epoch 7, Loss: 0.8923308253288269\n",
            "Epoch 7, Loss: 1.0005342960357666\n",
            "Epoch 7, Loss: 0.9388878345489502\n",
            "Epoch 7, Loss: 0.9446029663085938\n",
            "Epoch 7, Loss: 0.8839583992958069\n",
            "Epoch 7, Loss: 0.8703729510307312\n",
            "Epoch 7, Loss: 0.9502946734428406\n",
            "Epoch 7, Loss: 0.8907526135444641\n",
            "Epoch 7, Loss: 1.063248634338379\n",
            "Epoch 7, Loss: 1.1115756034851074\n",
            "Epoch 7, Loss: 0.9192306995391846\n",
            "Epoch 7, Loss: 0.8814857602119446\n",
            "Epoch 7, Loss: 0.8773219585418701\n",
            "Epoch 7, Loss: 0.8843463063240051\n",
            "Epoch 7, Loss: 0.9124658107757568\n",
            "Epoch 7, Loss: 0.8500047326087952\n",
            "Epoch 7, Loss: 0.9420908093452454\n",
            "Epoch 7, Loss: 0.8912718892097473\n",
            "Epoch 7, Loss: 0.9203141927719116\n",
            "Epoch 7, Loss: 0.8653120398521423\n",
            "Epoch 7, Loss: 1.261782169342041\n",
            "Epoch 7, Loss: 0.8781501650810242\n",
            "Epoch 7, Loss: 0.8600090146064758\n",
            "Epoch 7, Loss: 0.9494777917861938\n",
            "Epoch 7, Loss: 0.9986650347709656\n",
            "Epoch 7, Loss: 0.9117390513420105\n",
            "Epoch 7, Loss: 0.9052891731262207\n",
            "Epoch 7, Loss: 0.9470709562301636\n",
            "Epoch 7, Loss: 0.8893427848815918\n",
            "Epoch 7, Loss: 0.8744919896125793\n",
            "Epoch 7, Loss: 0.8770645260810852\n",
            "Epoch 7, Loss: 0.8539173603057861\n",
            "Epoch 7, Loss: 0.8853638768196106\n",
            "Epoch 7, Loss: 0.946777880191803\n",
            "Epoch 7, Loss: 0.8567233681678772\n",
            "Epoch 7, Loss: 0.8823111057281494\n",
            "Epoch 7, Loss: 0.9902023077011108\n",
            "Epoch 7, Loss: 0.8927313685417175\n",
            "Epoch 7, Loss: 0.9837530851364136\n",
            "Epoch 7, Loss: 0.9096436500549316\n",
            "Epoch 7, Loss: 0.9956003427505493\n",
            "Epoch 7, Loss: 0.8574942350387573\n",
            "Epoch 7, Loss: 1.1003025770187378\n",
            "Epoch 7, Loss: 1.1020766496658325\n",
            "Epoch 7, Loss: 1.0010466575622559\n",
            "Epoch 7, Loss: 0.9432045817375183\n",
            "Epoch 7, Loss: 0.8940775394439697\n",
            "Epoch 7, Loss: 0.8639500141143799\n",
            "Epoch 7, Loss: 1.011115550994873\n",
            "Epoch 7, Loss: 0.8716717958450317\n",
            "Epoch 8, Loss: 0.9398694038391113\n",
            "Epoch 8, Loss: 0.9572769403457642\n",
            "Epoch 8, Loss: 0.9380530714988708\n",
            "Epoch 8, Loss: 0.9287897944450378\n",
            "Epoch 8, Loss: 0.9461543560028076\n",
            "Epoch 8, Loss: 0.8731015920639038\n",
            "Epoch 8, Loss: 0.9058057069778442\n",
            "Epoch 8, Loss: 0.9897727966308594\n",
            "Epoch 8, Loss: 0.9264897108078003\n",
            "Epoch 8, Loss: 0.9018526673316956\n",
            "Epoch 8, Loss: 0.9452811479568481\n",
            "Epoch 8, Loss: 0.9204590320587158\n",
            "Epoch 8, Loss: 0.9854395985603333\n",
            "Epoch 8, Loss: 0.8905612826347351\n",
            "Epoch 8, Loss: 0.9192308783531189\n",
            "Epoch 8, Loss: 0.8961753249168396\n",
            "Epoch 8, Loss: 0.9414354562759399\n",
            "Epoch 8, Loss: 0.8559771776199341\n",
            "Epoch 8, Loss: 0.95723557472229\n",
            "Epoch 8, Loss: 0.9487242102622986\n",
            "Epoch 8, Loss: 0.9221977591514587\n",
            "Epoch 8, Loss: 0.883394181728363\n",
            "Epoch 8, Loss: 0.9020189046859741\n",
            "Epoch 8, Loss: 0.9137353301048279\n",
            "Epoch 8, Loss: 0.94893479347229\n",
            "Epoch 8, Loss: 0.9812383055686951\n",
            "Epoch 8, Loss: 0.9225569367408752\n",
            "Epoch 8, Loss: 0.9586601257324219\n",
            "Epoch 8, Loss: 0.9831485152244568\n",
            "Epoch 8, Loss: 0.9266924262046814\n",
            "Epoch 8, Loss: 1.1494089365005493\n",
            "Epoch 8, Loss: 0.8824945092201233\n",
            "Epoch 8, Loss: 0.8971459865570068\n",
            "Epoch 8, Loss: 0.8424685597419739\n",
            "Epoch 8, Loss: 0.9004852175712585\n",
            "Epoch 8, Loss: 0.8793948292732239\n",
            "Epoch 8, Loss: 0.8961326479911804\n",
            "Epoch 8, Loss: 0.9275099635124207\n",
            "Epoch 8, Loss: 0.8619481921195984\n",
            "Epoch 8, Loss: 0.9083173871040344\n",
            "Epoch 8, Loss: 0.8807560801506042\n",
            "Epoch 8, Loss: 0.9194534420967102\n",
            "Epoch 8, Loss: 1.0847299098968506\n",
            "Epoch 8, Loss: 0.8469708561897278\n",
            "Epoch 8, Loss: 0.9209178686141968\n",
            "Epoch 8, Loss: 1.0054115056991577\n",
            "Epoch 8, Loss: 0.8666226267814636\n",
            "Epoch 8, Loss: 0.892362117767334\n",
            "Epoch 8, Loss: 0.9156706929206848\n",
            "Epoch 8, Loss: 0.8965863585472107\n",
            "Epoch 8, Loss: 0.9186158180236816\n",
            "Epoch 8, Loss: 0.9593080282211304\n",
            "Epoch 8, Loss: 0.8647583723068237\n",
            "Epoch 8, Loss: 0.9311918020248413\n",
            "Epoch 8, Loss: 0.9440127611160278\n",
            "Epoch 8, Loss: 0.9174306988716125\n",
            "Epoch 8, Loss: 0.9313101768493652\n",
            "Epoch 8, Loss: 0.9553435444831848\n",
            "Epoch 8, Loss: 0.8842283487319946\n",
            "Epoch 8, Loss: 1.1000275611877441\n",
            "Epoch 8, Loss: 0.8869715929031372\n",
            "Epoch 8, Loss: 0.8822108507156372\n",
            "Epoch 8, Loss: 0.8859214186668396\n",
            "Epoch 8, Loss: 0.9355971217155457\n",
            "Epoch 8, Loss: 0.9109534621238708\n",
            "Epoch 8, Loss: 0.8738167881965637\n",
            "Epoch 8, Loss: 1.002541184425354\n",
            "Epoch 8, Loss: 0.9987329244613647\n",
            "Epoch 8, Loss: 0.9503482580184937\n",
            "Epoch 8, Loss: 0.8691698908805847\n",
            "Epoch 8, Loss: 0.853955864906311\n",
            "Epoch 8, Loss: 0.8788545727729797\n",
            "Epoch 8, Loss: 0.9289997220039368\n",
            "Epoch 8, Loss: 0.9030598402023315\n",
            "Epoch 8, Loss: 0.9500912427902222\n",
            "Epoch 8, Loss: 0.8854685425758362\n",
            "Epoch 8, Loss: 0.9081652164459229\n",
            "Epoch 8, Loss: 0.9304931163787842\n",
            "Epoch 8, Loss: 0.8948004841804504\n",
            "Epoch 8, Loss: 1.0017906427383423\n",
            "Epoch 8, Loss: 0.9897504448890686\n",
            "Epoch 8, Loss: 0.9400777816772461\n",
            "Epoch 8, Loss: 0.9225137233734131\n",
            "Epoch 8, Loss: 0.8968874216079712\n",
            "Epoch 8, Loss: 1.1739091873168945\n",
            "Epoch 8, Loss: 0.9010280966758728\n",
            "Epoch 8, Loss: 0.9005345702171326\n",
            "Epoch 8, Loss: 0.8692975044250488\n",
            "Epoch 8, Loss: 0.9125124216079712\n",
            "Epoch 8, Loss: 0.9370084404945374\n",
            "Epoch 8, Loss: 0.9067097306251526\n",
            "Epoch 8, Loss: 0.9250599145889282\n",
            "Epoch 8, Loss: 0.8764481544494629\n",
            "Epoch 8, Loss: 0.8734551072120667\n",
            "Epoch 8, Loss: 0.8591581583023071\n",
            "Epoch 8, Loss: 0.8966535329818726\n",
            "Epoch 8, Loss: 0.8815410137176514\n",
            "Epoch 8, Loss: 0.9166432619094849\n",
            "Epoch 8, Loss: 0.9204254746437073\n",
            "Epoch 8, Loss: 0.9375513195991516\n",
            "Epoch 8, Loss: 0.8584368228912354\n",
            "Epoch 8, Loss: 0.818930983543396\n",
            "Epoch 8, Loss: 0.9056053757667542\n",
            "Epoch 8, Loss: 0.8590539693832397\n",
            "Epoch 8, Loss: 0.9270967245101929\n",
            "Epoch 8, Loss: 0.9211578369140625\n",
            "Epoch 8, Loss: 0.9812292456626892\n",
            "Epoch 8, Loss: 0.9942745566368103\n",
            "Epoch 8, Loss: 0.9093538522720337\n",
            "Epoch 8, Loss: 0.9960258603096008\n",
            "Epoch 8, Loss: 0.9000130891799927\n",
            "Epoch 8, Loss: 0.8493337035179138\n",
            "Epoch 8, Loss: 0.9010841846466064\n",
            "Epoch 8, Loss: 0.9311327338218689\n",
            "Epoch 8, Loss: 0.9159826636314392\n",
            "Epoch 8, Loss: 0.903312623500824\n",
            "Epoch 8, Loss: 0.8974828124046326\n",
            "Epoch 8, Loss: 1.0346057415008545\n",
            "Epoch 8, Loss: 0.9024631977081299\n",
            "Epoch 8, Loss: 0.9441386461257935\n",
            "Epoch 8, Loss: 0.9615620970726013\n",
            "Epoch 8, Loss: 0.9747567176818848\n",
            "Epoch 8, Loss: 1.079176902770996\n",
            "Epoch 8, Loss: 0.88693767786026\n",
            "Epoch 8, Loss: 0.8936307430267334\n",
            "Epoch 8, Loss: 0.9221252799034119\n",
            "Epoch 8, Loss: 1.0999294519424438\n",
            "Epoch 8, Loss: 0.8960971832275391\n",
            "Epoch 8, Loss: 1.1669058799743652\n",
            "Epoch 8, Loss: 0.8833481669425964\n",
            "Epoch 8, Loss: 0.9834988713264465\n",
            "Epoch 8, Loss: 0.9619301557540894\n",
            "Epoch 8, Loss: 0.8788672685623169\n",
            "Epoch 8, Loss: 0.8517777323722839\n",
            "Epoch 8, Loss: 1.0790654420852661\n",
            "Epoch 8, Loss: 1.011008381843567\n",
            "Epoch 8, Loss: 0.9323765635490417\n",
            "Epoch 8, Loss: 0.8847898840904236\n",
            "Epoch 8, Loss: 0.9161175489425659\n",
            "Epoch 8, Loss: 1.0354245901107788\n",
            "Epoch 8, Loss: 0.9163315296173096\n",
            "Epoch 8, Loss: 0.9996589422225952\n",
            "Epoch 8, Loss: 0.9361432194709778\n",
            "Epoch 8, Loss: 0.8928478956222534\n",
            "Epoch 8, Loss: 0.8581897020339966\n",
            "Epoch 8, Loss: 0.8575659990310669\n",
            "Epoch 8, Loss: 1.0366982221603394\n",
            "Epoch 8, Loss: 0.8832290768623352\n",
            "Epoch 8, Loss: 0.9452551007270813\n",
            "Epoch 8, Loss: 1.0319502353668213\n",
            "Epoch 8, Loss: 0.9523483514785767\n",
            "Epoch 8, Loss: 0.9017901420593262\n",
            "Epoch 8, Loss: 0.8706058263778687\n",
            "Epoch 8, Loss: 0.9180728197097778\n",
            "Epoch 8, Loss: 0.9200971722602844\n",
            "Epoch 8, Loss: 0.9106733202934265\n",
            "Epoch 8, Loss: 0.8723031282424927\n",
            "Epoch 8, Loss: 0.8625448346138\n",
            "Epoch 8, Loss: 0.8632915019989014\n",
            "Epoch 8, Loss: 0.8592239022254944\n",
            "Epoch 8, Loss: 0.9823145866394043\n",
            "Epoch 8, Loss: 0.8745863437652588\n",
            "Epoch 8, Loss: 0.9569936990737915\n",
            "Epoch 8, Loss: 0.8973764181137085\n",
            "Epoch 8, Loss: 0.9585914611816406\n",
            "Epoch 8, Loss: 0.8725477457046509\n",
            "Epoch 8, Loss: 0.9126527309417725\n",
            "Epoch 8, Loss: 0.8545748591423035\n",
            "Epoch 8, Loss: 0.883491039276123\n",
            "Epoch 8, Loss: 0.8942702412605286\n",
            "Epoch 8, Loss: 0.8827273845672607\n",
            "Epoch 8, Loss: 0.8893039226531982\n",
            "Epoch 8, Loss: 0.8911029100418091\n",
            "Epoch 8, Loss: 0.8675858378410339\n",
            "Epoch 8, Loss: 0.9026637077331543\n",
            "Epoch 8, Loss: 0.8759168386459351\n",
            "Epoch 9, Loss: 0.9132179617881775\n",
            "Epoch 9, Loss: 1.0062235593795776\n",
            "Epoch 9, Loss: 0.8537999987602234\n",
            "Epoch 9, Loss: 0.8702137470245361\n",
            "Epoch 9, Loss: 1.0749741792678833\n",
            "Epoch 9, Loss: 0.8552227020263672\n",
            "Epoch 9, Loss: 0.8902490139007568\n",
            "Epoch 9, Loss: 0.9136244654655457\n",
            "Epoch 9, Loss: 1.0239195823669434\n",
            "Epoch 9, Loss: 0.8466998338699341\n",
            "Epoch 9, Loss: 1.0152826309204102\n",
            "Epoch 9, Loss: 0.9938427209854126\n",
            "Epoch 9, Loss: 0.9061957001686096\n",
            "Epoch 9, Loss: 0.8828123807907104\n",
            "Epoch 9, Loss: 1.0108435153961182\n",
            "Epoch 9, Loss: 0.9039640426635742\n",
            "Epoch 9, Loss: 0.9840866923332214\n",
            "Epoch 9, Loss: 0.9181591868400574\n",
            "Epoch 9, Loss: 0.9428285956382751\n",
            "Epoch 9, Loss: 0.9714853167533875\n",
            "Epoch 9, Loss: 1.05349862575531\n",
            "Epoch 9, Loss: 0.9422659277915955\n",
            "Epoch 9, Loss: 0.8867762684822083\n",
            "Epoch 9, Loss: 0.9194315671920776\n",
            "Epoch 9, Loss: 0.8730369806289673\n",
            "Epoch 9, Loss: 0.8757364749908447\n",
            "Epoch 9, Loss: 1.0133144855499268\n",
            "Epoch 9, Loss: 0.8870957493782043\n",
            "Epoch 9, Loss: 0.8829172849655151\n",
            "Epoch 9, Loss: 0.9124351143836975\n",
            "Epoch 9, Loss: 0.9443364143371582\n",
            "Epoch 9, Loss: 0.8756920099258423\n",
            "Epoch 9, Loss: 0.8529790043830872\n",
            "Epoch 9, Loss: 0.9085609912872314\n",
            "Epoch 9, Loss: 0.8883833289146423\n",
            "Epoch 9, Loss: 0.9329621195793152\n",
            "Epoch 9, Loss: 0.8933703899383545\n",
            "Epoch 9, Loss: 0.8778305649757385\n",
            "Epoch 9, Loss: 0.9193980693817139\n",
            "Epoch 9, Loss: 0.8448855876922607\n",
            "Epoch 9, Loss: 0.8864776492118835\n",
            "Epoch 9, Loss: 0.8859723210334778\n",
            "Epoch 9, Loss: 0.9099031090736389\n",
            "Epoch 9, Loss: 0.9384695291519165\n",
            "Epoch 9, Loss: 0.9348165988922119\n",
            "Epoch 9, Loss: 0.9198565483093262\n",
            "Epoch 9, Loss: 0.9076434373855591\n",
            "Epoch 9, Loss: 0.8255672454833984\n",
            "Epoch 9, Loss: 0.9075215458869934\n",
            "Epoch 9, Loss: 0.9097435474395752\n",
            "Epoch 9, Loss: 1.0972940921783447\n",
            "Epoch 9, Loss: 0.9003085494041443\n",
            "Epoch 9, Loss: 0.8528432250022888\n",
            "Epoch 9, Loss: 0.904941201210022\n",
            "Epoch 9, Loss: 0.9101356267929077\n",
            "Epoch 9, Loss: 0.8743211627006531\n",
            "Epoch 9, Loss: 0.8771089911460876\n",
            "Epoch 9, Loss: 0.9212536811828613\n",
            "Epoch 9, Loss: 0.878903329372406\n",
            "Epoch 9, Loss: 0.847139835357666\n",
            "Epoch 9, Loss: 0.8798605799674988\n",
            "Epoch 9, Loss: 0.8545281291007996\n",
            "Epoch 9, Loss: 0.8866580724716187\n",
            "Epoch 9, Loss: 0.8915446996688843\n",
            "Epoch 9, Loss: 0.9791359305381775\n",
            "Epoch 9, Loss: 0.9607253074645996\n",
            "Epoch 9, Loss: 0.9449761509895325\n",
            "Epoch 9, Loss: 0.886162519454956\n",
            "Epoch 9, Loss: 0.958730161190033\n",
            "Epoch 9, Loss: 0.9023431539535522\n",
            "Epoch 9, Loss: 0.9236443638801575\n",
            "Epoch 9, Loss: 0.9047064781188965\n",
            "Epoch 9, Loss: 0.8977108001708984\n",
            "Epoch 9, Loss: 0.9045842885971069\n",
            "Epoch 9, Loss: 0.9755468368530273\n",
            "Epoch 9, Loss: 0.9055094718933105\n",
            "Epoch 9, Loss: 0.8408094048500061\n",
            "Epoch 9, Loss: 0.8926334977149963\n",
            "Epoch 9, Loss: 0.9120844006538391\n",
            "Epoch 9, Loss: 1.0242842435836792\n",
            "Epoch 9, Loss: 0.9347554445266724\n",
            "Epoch 9, Loss: 0.9186155796051025\n",
            "Epoch 9, Loss: 0.8963614702224731\n",
            "Epoch 9, Loss: 0.9598642587661743\n",
            "Epoch 9, Loss: 0.904688835144043\n",
            "Epoch 9, Loss: 0.9695383906364441\n",
            "Epoch 9, Loss: 0.9005410671234131\n",
            "Epoch 9, Loss: 0.8774868845939636\n",
            "Epoch 9, Loss: 0.9124974012374878\n",
            "Epoch 9, Loss: 1.0124688148498535\n",
            "Epoch 9, Loss: 0.8851526975631714\n",
            "Epoch 9, Loss: 0.8417119383811951\n",
            "Epoch 9, Loss: 0.9341979026794434\n",
            "Epoch 9, Loss: 0.910302996635437\n",
            "Epoch 9, Loss: 0.8908466100692749\n",
            "Epoch 9, Loss: 0.8649982213973999\n",
            "Epoch 9, Loss: 0.9793802499771118\n",
            "Epoch 9, Loss: 0.9135616421699524\n",
            "Epoch 9, Loss: 0.8847755789756775\n",
            "Epoch 9, Loss: 0.9130971431732178\n",
            "Epoch 9, Loss: 0.8938009142875671\n",
            "Epoch 9, Loss: 1.0251150131225586\n",
            "Epoch 9, Loss: 0.885521411895752\n",
            "Epoch 9, Loss: 0.8803398609161377\n",
            "Epoch 9, Loss: 0.9078413844108582\n",
            "Epoch 9, Loss: 0.9048660397529602\n",
            "Epoch 9, Loss: 0.8731748461723328\n",
            "Epoch 9, Loss: 0.9221245646476746\n",
            "Epoch 9, Loss: 0.9646000266075134\n",
            "Epoch 9, Loss: 0.8980423212051392\n",
            "Epoch 9, Loss: 0.8889479637145996\n",
            "Epoch 9, Loss: 0.9022656083106995\n",
            "Epoch 9, Loss: 0.8792405724525452\n",
            "Epoch 9, Loss: 0.8783280849456787\n",
            "Epoch 9, Loss: 0.8820452690124512\n",
            "Epoch 9, Loss: 1.0255674123764038\n",
            "Epoch 9, Loss: 0.897756040096283\n",
            "Epoch 9, Loss: 0.9130427241325378\n",
            "Epoch 9, Loss: 0.9014217853546143\n",
            "Epoch 9, Loss: 0.9299847483634949\n",
            "Epoch 9, Loss: 0.988674521446228\n",
            "Epoch 9, Loss: 0.8972620368003845\n",
            "Epoch 9, Loss: 0.9037115573883057\n",
            "Epoch 9, Loss: 1.1724178791046143\n",
            "Epoch 9, Loss: 1.0907833576202393\n",
            "Epoch 9, Loss: 0.9433141350746155\n",
            "Epoch 9, Loss: 0.8814581632614136\n",
            "Epoch 9, Loss: 0.9541429877281189\n",
            "Epoch 9, Loss: 0.8871853947639465\n",
            "Epoch 9, Loss: 0.8758440613746643\n",
            "Epoch 9, Loss: 0.9105119705200195\n",
            "Epoch 9, Loss: 0.9527018666267395\n",
            "Epoch 9, Loss: 0.9094554781913757\n",
            "Epoch 9, Loss: 0.9760902523994446\n",
            "Epoch 9, Loss: 0.9675019979476929\n",
            "Epoch 9, Loss: 0.9963597655296326\n",
            "Epoch 9, Loss: 0.8955848813056946\n",
            "Epoch 9, Loss: 0.941018283367157\n",
            "Epoch 9, Loss: 1.0235987901687622\n",
            "Epoch 9, Loss: 0.8748632669448853\n",
            "Epoch 9, Loss: 0.9023855924606323\n",
            "Epoch 9, Loss: 0.9351447820663452\n",
            "Epoch 9, Loss: 0.9332612156867981\n",
            "Epoch 9, Loss: 0.8687702417373657\n",
            "Epoch 9, Loss: 0.9086822271347046\n",
            "Epoch 9, Loss: 0.8475773930549622\n",
            "Epoch 9, Loss: 0.8797376751899719\n",
            "Epoch 9, Loss: 0.9834189414978027\n",
            "Epoch 9, Loss: 0.8496744632720947\n",
            "Epoch 9, Loss: 0.8942034244537354\n",
            "Epoch 9, Loss: 0.9289997220039368\n",
            "Epoch 9, Loss: 0.9492356181144714\n",
            "Epoch 9, Loss: 0.969074547290802\n",
            "Epoch 9, Loss: 0.8646613359451294\n",
            "Epoch 9, Loss: 0.9471990466117859\n",
            "Epoch 9, Loss: 0.898965060710907\n",
            "Epoch 9, Loss: 0.9647597670555115\n",
            "Epoch 9, Loss: 0.9854627251625061\n",
            "Epoch 9, Loss: 0.9447066187858582\n",
            "Epoch 9, Loss: 1.1205601692199707\n",
            "Epoch 9, Loss: 0.9061601758003235\n",
            "Epoch 9, Loss: 0.9079734683036804\n",
            "Epoch 9, Loss: 0.878751814365387\n",
            "Epoch 9, Loss: 0.8920418620109558\n",
            "Epoch 9, Loss: 1.0270729064941406\n",
            "Epoch 9, Loss: 1.0676181316375732\n",
            "Epoch 9, Loss: 0.9126494526863098\n",
            "Epoch 9, Loss: 0.9106785655021667\n",
            "Epoch 9, Loss: 0.8957307934761047\n",
            "Epoch 9, Loss: 0.915377140045166\n",
            "Epoch 9, Loss: 1.0244117975234985\n",
            "Epoch 9, Loss: 0.9389489889144897\n",
            "Epoch 9, Loss: 1.0306386947631836\n",
            "Epoch 9, Loss: 1.1185379028320312\n",
            "Epoch 9, Loss: 0.9763461947441101\n",
            "Epoch 9, Loss: 0.9249837398529053\n",
            "Epoch 10, Loss: 0.8777667284011841\n",
            "Epoch 10, Loss: 1.013014793395996\n",
            "Epoch 10, Loss: 0.9253581166267395\n",
            "Epoch 10, Loss: 0.8683642745018005\n",
            "Epoch 10, Loss: 0.906764566898346\n",
            "Epoch 10, Loss: 0.8808254599571228\n",
            "Epoch 10, Loss: 0.8891366720199585\n",
            "Epoch 10, Loss: 0.9366902709007263\n",
            "Epoch 10, Loss: 0.9058160781860352\n",
            "Epoch 10, Loss: 0.9009380340576172\n",
            "Epoch 10, Loss: 1.0062837600708008\n",
            "Epoch 10, Loss: 0.8929846286773682\n",
            "Epoch 10, Loss: 0.8966081142425537\n",
            "Epoch 10, Loss: 1.0408782958984375\n",
            "Epoch 10, Loss: 0.9184965491294861\n",
            "Epoch 10, Loss: 0.8736379146575928\n",
            "Epoch 10, Loss: 0.900763750076294\n",
            "Epoch 10, Loss: 1.0026037693023682\n",
            "Epoch 10, Loss: 0.8897072672843933\n",
            "Epoch 10, Loss: 0.8608265519142151\n",
            "Epoch 10, Loss: 0.9143423438072205\n",
            "Epoch 10, Loss: 0.8706563711166382\n",
            "Epoch 10, Loss: 0.9286460280418396\n",
            "Epoch 10, Loss: 0.8678230047225952\n",
            "Epoch 10, Loss: 0.9343893527984619\n",
            "Epoch 10, Loss: 0.8716395497322083\n",
            "Epoch 10, Loss: 0.8815622329711914\n",
            "Epoch 10, Loss: 0.9368790984153748\n",
            "Epoch 10, Loss: 0.9469767808914185\n",
            "Epoch 10, Loss: 0.9172422289848328\n",
            "Epoch 10, Loss: 1.1263598203659058\n",
            "Epoch 10, Loss: 1.0620354413986206\n",
            "Epoch 10, Loss: 0.898221492767334\n",
            "Epoch 10, Loss: 0.9463152885437012\n",
            "Epoch 10, Loss: 0.9158571362495422\n",
            "Epoch 10, Loss: 0.9067464470863342\n",
            "Epoch 10, Loss: 0.9128804206848145\n",
            "Epoch 10, Loss: 1.099428653717041\n",
            "Epoch 10, Loss: 0.9066978693008423\n",
            "Epoch 10, Loss: 0.8803728222846985\n",
            "Epoch 10, Loss: 0.8630862236022949\n",
            "Epoch 10, Loss: 0.8976980447769165\n",
            "Epoch 10, Loss: 0.8600847721099854\n",
            "Epoch 10, Loss: 0.9496951699256897\n",
            "Epoch 10, Loss: 0.8683182597160339\n",
            "Epoch 10, Loss: 0.8972252011299133\n",
            "Epoch 10, Loss: 1.0377367734909058\n",
            "Epoch 10, Loss: 0.8944723606109619\n",
            "Epoch 10, Loss: 0.9321429133415222\n",
            "Epoch 10, Loss: 1.189514398574829\n",
            "Epoch 10, Loss: 1.0626775026321411\n",
            "Epoch 10, Loss: 0.8615002036094666\n",
            "Epoch 10, Loss: 0.9776811003684998\n",
            "Epoch 10, Loss: 0.921186625957489\n",
            "Epoch 10, Loss: 1.0830154418945312\n",
            "Epoch 10, Loss: 0.8784410953521729\n",
            "Epoch 10, Loss: 0.8917926549911499\n",
            "Epoch 10, Loss: 0.9076223969459534\n",
            "Epoch 10, Loss: 0.9211530685424805\n",
            "Epoch 10, Loss: 0.8464096188545227\n",
            "Epoch 10, Loss: 0.957858145236969\n",
            "Epoch 10, Loss: 0.8659129738807678\n",
            "Epoch 10, Loss: 0.8647777438163757\n",
            "Epoch 10, Loss: 1.017199158668518\n",
            "Epoch 10, Loss: 0.9305087327957153\n",
            "Epoch 10, Loss: 0.995136559009552\n",
            "Epoch 10, Loss: 0.8929556012153625\n",
            "Epoch 10, Loss: 0.9404550790786743\n",
            "Epoch 10, Loss: 0.8834570050239563\n",
            "Epoch 10, Loss: 0.912655770778656\n",
            "Epoch 10, Loss: 0.9169461131095886\n",
            "Epoch 10, Loss: 0.8952845931053162\n",
            "Epoch 10, Loss: 0.8748916983604431\n",
            "Epoch 10, Loss: 0.8983855247497559\n",
            "Epoch 10, Loss: 0.9117674827575684\n",
            "Epoch 10, Loss: 0.8937228322029114\n",
            "Epoch 10, Loss: 0.9126607775688171\n",
            "Epoch 10, Loss: 0.9839674830436707\n",
            "Epoch 10, Loss: 0.8862969875335693\n",
            "Epoch 10, Loss: 0.9502509832382202\n",
            "Epoch 10, Loss: 0.9915429949760437\n",
            "Epoch 10, Loss: 0.9182906150817871\n",
            "Epoch 10, Loss: 0.8518484830856323\n",
            "Epoch 10, Loss: 0.9637007713317871\n",
            "Epoch 10, Loss: 0.8814169764518738\n",
            "Epoch 10, Loss: 0.8967109322547913\n",
            "Epoch 10, Loss: 0.927160382270813\n",
            "Epoch 10, Loss: 0.9228169322013855\n",
            "Epoch 10, Loss: 1.0164251327514648\n",
            "Epoch 10, Loss: 0.868708610534668\n",
            "Epoch 10, Loss: 0.8340701460838318\n",
            "Epoch 10, Loss: 0.8549400568008423\n",
            "Epoch 10, Loss: 0.8917567729949951\n",
            "Epoch 10, Loss: 0.9704381227493286\n",
            "Epoch 10, Loss: 0.8865689635276794\n",
            "Epoch 10, Loss: 0.9071407318115234\n",
            "Epoch 10, Loss: 0.9289048910140991\n",
            "Epoch 10, Loss: 0.8974452614784241\n",
            "Epoch 10, Loss: 0.9289764761924744\n",
            "Epoch 10, Loss: 0.8903807997703552\n",
            "Epoch 10, Loss: 0.926182210445404\n",
            "Epoch 10, Loss: 0.9070030450820923\n",
            "Epoch 10, Loss: 0.8592398166656494\n",
            "Epoch 10, Loss: 0.8783674240112305\n",
            "Epoch 10, Loss: 0.9542838931083679\n",
            "Epoch 10, Loss: 0.9070751070976257\n",
            "Epoch 10, Loss: 0.9017856121063232\n",
            "Epoch 10, Loss: 1.0516653060913086\n",
            "Epoch 10, Loss: 0.9060884714126587\n",
            "Epoch 10, Loss: 0.8781225085258484\n",
            "Epoch 10, Loss: 1.0536688566207886\n",
            "Epoch 10, Loss: 0.8773195743560791\n",
            "Epoch 10, Loss: 0.9583766460418701\n",
            "Epoch 10, Loss: 0.869043231010437\n",
            "Epoch 10, Loss: 1.0315202474594116\n",
            "Epoch 10, Loss: 1.0263553857803345\n",
            "Epoch 10, Loss: 0.91620272397995\n",
            "Epoch 10, Loss: 0.9085351228713989\n",
            "Epoch 10, Loss: 0.9735891222953796\n",
            "Epoch 10, Loss: 0.9524201154708862\n",
            "Epoch 10, Loss: 0.8744676113128662\n",
            "Epoch 10, Loss: 0.91678386926651\n",
            "Epoch 10, Loss: 0.867241621017456\n",
            "Epoch 10, Loss: 0.9187226891517639\n",
            "Epoch 10, Loss: 0.8995667695999146\n",
            "Epoch 10, Loss: 0.9604885578155518\n",
            "Epoch 10, Loss: 0.9341112375259399\n",
            "Epoch 10, Loss: 1.0478425025939941\n",
            "Epoch 10, Loss: 0.8949686288833618\n",
            "Epoch 10, Loss: 0.9190559983253479\n",
            "Epoch 10, Loss: 1.0054404735565186\n",
            "Epoch 10, Loss: 0.8779469728469849\n",
            "Epoch 10, Loss: 0.9378814697265625\n",
            "Epoch 10, Loss: 0.8501377701759338\n",
            "Epoch 10, Loss: 0.8471529483795166\n",
            "Epoch 10, Loss: 0.903028130531311\n",
            "Epoch 10, Loss: 0.957968533039093\n",
            "Epoch 10, Loss: 0.9135966300964355\n",
            "Epoch 10, Loss: 0.9234115481376648\n",
            "Epoch 10, Loss: 0.9239040017127991\n",
            "Epoch 10, Loss: 0.935625433921814\n",
            "Epoch 10, Loss: 0.8703635931015015\n",
            "Epoch 10, Loss: 0.8548547029495239\n",
            "Epoch 10, Loss: 0.8887671828269958\n",
            "Epoch 10, Loss: 0.9407544732093811\n",
            "Epoch 10, Loss: 0.9988799691200256\n",
            "Epoch 10, Loss: 0.8994714617729187\n",
            "Epoch 10, Loss: 0.8611712455749512\n",
            "Epoch 10, Loss: 0.8734621405601501\n",
            "Epoch 10, Loss: 0.9182687997817993\n",
            "Epoch 10, Loss: 0.9424124956130981\n",
            "Epoch 10, Loss: 0.9177448749542236\n",
            "Epoch 10, Loss: 0.8727670907974243\n",
            "Epoch 10, Loss: 0.8551033139228821\n",
            "Epoch 10, Loss: 0.972481369972229\n",
            "Epoch 10, Loss: 0.8801600337028503\n",
            "Epoch 10, Loss: 0.921413242816925\n",
            "Epoch 10, Loss: 0.9443201422691345\n",
            "Epoch 10, Loss: 0.8648476600646973\n",
            "Epoch 10, Loss: 0.8777714967727661\n",
            "Epoch 10, Loss: 0.8853510022163391\n",
            "Epoch 10, Loss: 0.9060041308403015\n",
            "Epoch 10, Loss: 0.8736286163330078\n",
            "Epoch 10, Loss: 1.0247092247009277\n",
            "Epoch 10, Loss: 0.9566402435302734\n",
            "Epoch 10, Loss: 0.9465628862380981\n",
            "Epoch 10, Loss: 0.894284188747406\n",
            "Epoch 10, Loss: 1.162627100944519\n",
            "Epoch 10, Loss: 0.9041390419006348\n",
            "Epoch 10, Loss: 0.8996987342834473\n",
            "Epoch 10, Loss: 0.960713267326355\n",
            "Epoch 10, Loss: 0.896612286567688\n",
            "Epoch 10, Loss: 0.877629280090332\n",
            "Epoch 10, Loss: 1.0903180837631226\n",
            "Epoch 10, Loss: 0.9686972498893738\n",
            "Epoch 10, Loss: 0.8937453031539917\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Model Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Faster R-CNN pre-trained on COCO\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Modify the classifier head for 2 classes (tumor + background)\n",
        "num_classes = 2\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "    in_features, num_classes\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer with a lower learning rate\n",
        "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10  # Set how many epochs you want to train\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for images_batch, targets_batch in train_loader:\n",
        "        # Move images to device\n",
        "        images_batch = list(image.to(device) for image in images_batch)\n",
        "\n",
        "        # Clean and move targets\n",
        "        cleaned_targets = []\n",
        "        for i in range(len(targets_batch)):\n",
        "            boxes = targets_batch[i]['boxes']\n",
        "            labels = targets_batch[i]['labels']\n",
        "\n",
        "            valid_boxes = []\n",
        "            valid_labels = []\n",
        "\n",
        "            for box, label in zip(boxes, labels):\n",
        "                x_min, y_min, x_max, y_max = box\n",
        "                width = x_max - x_min\n",
        "                height = y_max - y_min\n",
        "\n",
        "                if width > 0 and height > 0:\n",
        "                    valid_boxes.append(box)\n",
        "                    valid_labels.append(label)\n",
        "\n",
        "            if len(valid_boxes) == 0:\n",
        "                cleaned_target = {\n",
        "                    'boxes': torch.empty((0, 4), dtype=torch.float32).to(device),\n",
        "                    'labels': torch.empty((0,), dtype=torch.int64).to(device)\n",
        "                }\n",
        "            else:\n",
        "                cleaned_target = {\n",
        "                    'boxes': torch.stack(valid_boxes).float().to(device),\n",
        "                    'labels': torch.tensor(valid_labels, dtype=torch.int64).to(device)\n",
        "                }\n",
        "\n",
        "            cleaned_targets.append(cleaned_target)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images_batch, cleaned_targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {losses.item()}\")\n",
        "\n",
        "    # Step the scheduler\n",
        "    lr_scheduler.step()\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fS4sjjtYMDg5",
        "outputId": "faae4fdf-97d1-4960-b014-7789d9e34052"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [182, 17600]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3a395506a1fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Now calculate Precision, Recall, and F1-Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2245\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \"\"\"\n\u001b[0;32m-> 2247\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2248\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \"\"\"\n\u001b[1;32m   1829\u001b[0m     \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattach_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m     97\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [182, 17600]"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Initialize lists for the true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# Loop through the test data\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for images_batch, targets_batch in test_loader:\n",
        "        images_batch = images_batch.to(device)\n",
        "        targets_batch = [{k: v.to(device) for k, v in t.items()} for t in targets_batch]\n",
        "\n",
        "        # Get the predictions from the model\n",
        "        output = model(images_batch)\n",
        "\n",
        "        for i in range(len(output)):\n",
        "            # Get the predicted labels and the true labels\n",
        "            pred_boxes = output[i]['boxes']\n",
        "            pred_scores = output[i]['scores']\n",
        "            pred_labels_img = output[i]['labels']\n",
        "\n",
        "            true_labels_img = targets_batch[i]['labels']\n",
        "\n",
        "            # Use only the predictions with a high enough score (e.g., score > 0.5)\n",
        "            high_score_preds = pred_scores > 0.5\n",
        "            pred_labels_img = pred_labels_img[high_score_preds]\n",
        "\n",
        "            # Add the predicted labels and true labels to the respective lists\n",
        "            true_labels.extend(true_labels_img.cpu().numpy())\n",
        "            pred_labels.extend(pred_labels_img.cpu().numpy())\n",
        "\n",
        "# Now calculate Precision, Recall, and F1-Score\n",
        "if len(pred_labels) > 0 and len(true_labels) > 0:\n",
        "    precision = precision_score(true_labels, pred_labels, pos_label=1)\n",
        "    recall = recall_score(true_labels, pred_labels, pos_label=1)\n",
        "    f1 = f1_score(true_labels, pred_labels, pos_label=1)\n",
        "\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "else:\n",
        "    print(\"No predictions made or all predictions were negative.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
